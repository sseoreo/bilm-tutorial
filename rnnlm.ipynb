{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataset import LMDataset\n",
    "from vocab import Vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"ptb\"\n",
    "epochs = 10\n",
    "batch_length = 16\n",
    "batch_size = 8\n",
    "lr = .001\n",
    "\n",
    "n_layers = 2\n",
    "d_emb = 200\n",
    "d_hid = 250\n",
    "p_drop = 0.2\n",
    "\n",
    "interval_print = 200\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42068/42068 [00:00<00:00, 122330.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 50770), ('<unk>', 45020), ('N', 32481), ('of', 24400), ('to', 23638), ('a', 21196), ('in', 18000), ('and', 17474), (\"'s\", 9784), ('that', 8931)]\n",
      "end building vocab ...\n",
      "['<pad>', '<eos>', 'the', '<unk>', 'N', 'of', 'to', 'a', 'in', 'and']\n",
      "binarizing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/42068 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Vocab' object has no attribute 'bos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d061bf8f2f82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mvalidset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bilm-tutorial/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_dir, vocab, batch_size, split)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# bsz x (len(data)/bsz)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bilm-tutorial/dataset.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(self, split, bsz)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bilm-tutorial/vocab.py\u001b[0m in \u001b[0;36mencode_line\u001b[0;34m(self, line, add_bos, add_eos)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_eos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok2id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0madd_bos\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbos_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'bos'"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(data_dir)\n",
    "trainset = LMDataset(data_dir, vocab, batch_size, 'train')\n",
    "validset = LMDataset(data_dir, vocab, batch_size, 'valid')\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_length)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddomgs, embedding_dim, p_drop=0.):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddomgs, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.emb(input)\n",
    "        output = self.dropout(output)\n",
    "        return output         \n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, p_drop):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.n_classes = vocab_size\n",
    "        self.d_emb = embedding_dim\n",
    "\n",
    "\n",
    "        self.word_embedding = WordEmbedding(self.n_classes, self.d_emb, p_drop=p_drop)\n",
    "        self.layers = nn.GRU(self.d_emb, hidden_dim, n_layers, dropout=p_drop, batch_first=True)\n",
    "        self.proj_layer = nn.Linear(hidden_dim, self.n_classes)\n",
    "        \n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        # self.layer2 = nn.GRU(hidden_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        emb = self.word_embedding(input)\n",
    "        output, h = self.layers(emb)\n",
    "        output = self.drop(output)\n",
    "        output = self.proj_layer(output)\n",
    "        return output\n",
    "\n",
    "model = RNNLM(vocab_size=vocab.size, embedding_dim=d_emb, hidden_dim=d_hid, n_layers=n_layers, p_drop=p_drop)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = lr, # config.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # config.adam_epsilon  - default is 1e-8.\n",
    "                  )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr, gamma=0.95)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50] epochs training...\n",
      "n_iter:200 loss: 7.080 ppl: 1187.419\n",
      "n_iter:400 loss: 6.567 ppl: 710.913\n",
      "n_iter:600 loss: 6.426 ppl: 617.942\n",
      "n_iter:800 loss: 6.231 ppl: 508.165\n",
      "n_iter:1000 loss: 6.143 ppl: 465.624\n",
      "n_iter:1200 loss: 6.064 ppl: 430.188\n",
      "n_iter:1400 loss: 5.970 ppl: 391.658\n",
      "n_iter:1600 loss: 5.967 ppl: 390.277\n",
      "n_iter:1800 loss: 5.845 ppl: 345.358\n",
      "n_iter:2000 loss: 5.838 ppl: 343.170\n",
      "n_iter:2200 loss: 5.742 ppl: 311.633\n",
      "n_iter:2400 loss: 5.808 ppl: 332.856\n",
      "n_iter:2600 loss: 5.767 ppl: 319.666\n",
      "n_iter:2800 loss: 5.766 ppl: 319.188\n",
      "n_iter:3000 loss: 5.868 ppl: 353.491\n",
      "n_iter:3200 loss: 5.555 ppl: 258.414\n",
      "n_iter:3400 loss: 5.735 ppl: 309.389\n",
      "n_iter:3600 loss: 5.757 ppl: 316.520\n",
      "n_iter:3800 loss: 5.702 ppl: 299.533\n",
      "n_iter:4000 loss: 5.485 ppl: 240.939\n",
      "n_iter:4200 loss: 5.690 ppl: 295.831\n",
      "n_iter:4400 loss: 5.554 ppl: 258.172\n",
      "n_iter:4600 loss: 5.417 ppl: 225.101\n",
      "n_iter:4800 loss: 5.404 ppl: 222.318\n",
      "n_iter:5000 loss: 5.530 ppl: 252.259\n",
      "n_iter:5200 loss: 5.516 ppl: 248.748\n",
      "n_iter:5400 loss: 5.573 ppl: 263.155\n",
      "n_iter:5600 loss: 5.555 ppl: 258.505\n",
      "n_iter:5800 loss: 5.660 ppl: 287.169\n",
      "n_iter:6000 loss: 5.440 ppl: 230.333\n",
      "n_iter:6200 loss: 5.596 ppl: 269.472\n",
      "n_iter:6400 loss: 5.594 ppl: 268.768\n",
      "n_iter:6600 loss: 5.514 ppl: 248.263\n",
      "n_iter:6800 loss: 5.370 ppl: 214.799\n",
      "n_iter:7000 loss: 5.495 ppl: 243.453\n",
      "n_iter:7200 loss: 5.490 ppl: 242.239\n",
      "### find best mode ### 216.79046135152052\n",
      "validation vloss: 5.379 vppl: 216.790, best ppl: 216.790\n",
      "[1/50] epochs training...\n",
      "n_iter:7400 loss: 5.529 ppl: 251.770\n",
      "n_iter:7600 loss: 5.404 ppl: 222.184\n",
      "n_iter:7800 loss: 5.527 ppl: 251.489\n",
      "n_iter:8000 loss: 5.416 ppl: 224.956\n",
      "n_iter:8200 loss: 5.404 ppl: 222.228\n",
      "n_iter:8400 loss: 5.408 ppl: 223.221\n",
      "n_iter:8600 loss: 5.205 ppl: 182.154\n",
      "n_iter:8800 loss: 5.311 ppl: 202.589\n",
      "n_iter:9000 loss: 5.301 ppl: 200.461\n",
      "n_iter:9200 loss: 5.267 ppl: 193.762\n",
      "n_iter:9400 loss: 5.213 ppl: 183.695\n",
      "n_iter:9600 loss: 5.269 ppl: 194.160\n",
      "n_iter:9800 loss: 5.257 ppl: 191.823\n",
      "n_iter:10000 loss: 5.277 ppl: 195.782\n",
      "n_iter:10200 loss: 5.354 ppl: 211.446\n",
      "n_iter:10400 loss: 5.180 ppl: 177.608\n",
      "n_iter:10600 loss: 5.224 ppl: 185.748\n",
      "n_iter:10800 loss: 5.359 ppl: 212.560\n",
      "n_iter:11000 loss: 5.393 ppl: 219.810\n",
      "n_iter:11200 loss: 5.143 ppl: 171.164\n",
      "n_iter:11400 loss: 5.184 ppl: 178.436\n",
      "n_iter:11600 loss: 5.255 ppl: 191.517\n",
      "n_iter:11800 loss: 5.029 ppl: 152.851\n",
      "n_iter:12000 loss: 5.037 ppl: 154.072\n",
      "n_iter:12200 loss: 5.175 ppl: 176.802\n",
      "n_iter:12400 loss: 5.197 ppl: 180.646\n",
      "n_iter:12600 loss: 5.192 ppl: 179.826\n",
      "n_iter:12800 loss: 5.319 ppl: 204.210\n",
      "n_iter:13000 loss: 5.273 ppl: 194.955\n",
      "n_iter:13200 loss: 5.165 ppl: 175.016\n",
      "n_iter:13400 loss: 5.235 ppl: 187.699\n",
      "n_iter:13600 loss: 5.332 ppl: 206.762\n",
      "n_iter:13800 loss: 5.228 ppl: 186.333\n",
      "n_iter:14000 loss: 5.084 ppl: 161.458\n",
      "n_iter:14200 loss: 5.262 ppl: 192.935\n",
      "n_iter:14400 loss: 5.111 ppl: 165.809\n",
      "### find best mode ### 189.10608287882698\n",
      "validation vloss: 5.242 vppl: 189.106, best ppl: 189.106\n",
      "[2/50] epochs training...\n",
      "n_iter:14600 loss: 5.369 ppl: 214.654\n",
      "n_iter:14800 loss: 5.139 ppl: 170.570\n",
      "n_iter:15000 loss: 5.302 ppl: 200.759\n",
      "n_iter:15200 loss: 5.281 ppl: 196.472\n",
      "n_iter:15400 loss: 5.188 ppl: 179.050\n",
      "n_iter:15600 loss: 5.174 ppl: 176.687\n",
      "n_iter:15800 loss: 5.009 ppl: 149.785\n",
      "n_iter:16000 loss: 5.158 ppl: 173.824\n",
      "n_iter:16200 loss: 5.076 ppl: 160.191\n",
      "n_iter:16400 loss: 5.093 ppl: 162.882\n",
      "n_iter:16600 loss: 5.061 ppl: 157.779\n",
      "n_iter:16800 loss: 4.979 ppl: 145.349\n",
      "n_iter:17000 loss: 5.038 ppl: 154.115\n",
      "n_iter:17200 loss: 5.098 ppl: 163.634\n",
      "n_iter:17400 loss: 5.154 ppl: 173.057\n",
      "n_iter:17600 loss: 5.181 ppl: 177.889\n",
      "n_iter:17800 loss: 4.890 ppl: 132.948\n",
      "n_iter:18000 loss: 5.152 ppl: 172.692\n",
      "n_iter:18200 loss: 5.210 ppl: 183.008\n",
      "n_iter:18400 loss: 5.039 ppl: 154.311\n",
      "n_iter:18600 loss: 4.892 ppl: 133.283\n",
      "n_iter:18800 loss: 5.139 ppl: 170.545\n",
      "n_iter:19000 loss: 4.971 ppl: 144.211\n",
      "n_iter:19200 loss: 4.813 ppl: 123.145\n",
      "n_iter:19400 loss: 4.919 ppl: 136.838\n",
      "n_iter:19600 loss: 4.997 ppl: 147.987\n",
      "n_iter:19800 loss: 5.060 ppl: 157.567\n",
      "n_iter:20000 loss: 5.173 ppl: 176.380\n",
      "n_iter:20200 loss: 5.020 ppl: 151.451\n",
      "n_iter:20400 loss: 5.110 ppl: 165.671\n",
      "n_iter:20600 loss: 5.033 ppl: 153.397\n",
      "n_iter:20800 loss: 5.203 ppl: 181.808\n",
      "n_iter:21000 loss: 5.103 ppl: 164.543\n",
      "n_iter:21200 loss: 4.955 ppl: 141.857\n",
      "n_iter:21400 loss: 5.022 ppl: 151.680\n",
      "n_iter:21600 loss: 4.959 ppl: 142.496\n",
      "### find best mode ### 177.45510488443597\n",
      "validation vloss: 5.179 vppl: 177.455, best ppl: 177.455\n",
      "[3/50] epochs training...\n",
      "n_iter:21800 loss: 5.180 ppl: 177.703\n",
      "n_iter:22000 loss: 5.104 ppl: 164.701\n",
      "n_iter:22200 loss: 5.121 ppl: 167.472\n",
      "n_iter:22400 loss: 5.161 ppl: 174.371\n",
      "n_iter:22600 loss: 5.071 ppl: 159.307\n",
      "n_iter:22800 loss: 5.025 ppl: 152.151\n",
      "n_iter:23000 loss: 5.032 ppl: 153.267\n",
      "n_iter:23200 loss: 4.907 ppl: 135.291\n",
      "n_iter:23400 loss: 4.957 ppl: 142.223\n",
      "n_iter:23600 loss: 4.955 ppl: 141.835\n",
      "n_iter:23800 loss: 4.919 ppl: 136.839\n",
      "n_iter:24000 loss: 4.847 ppl: 127.325\n",
      "n_iter:24200 loss: 4.935 ppl: 139.058\n",
      "n_iter:24400 loss: 4.935 ppl: 139.095\n",
      "n_iter:24600 loss: 4.988 ppl: 146.620\n",
      "n_iter:24800 loss: 5.098 ppl: 163.626\n",
      "n_iter:25000 loss: 4.734 ppl: 113.709\n",
      "n_iter:25200 loss: 5.020 ppl: 151.393\n",
      "n_iter:25400 loss: 5.050 ppl: 156.040\n",
      "n_iter:25600 loss: 5.011 ppl: 150.065\n",
      "n_iter:25800 loss: 4.732 ppl: 113.505\n",
      "n_iter:26000 loss: 5.038 ppl: 154.166\n",
      "n_iter:26200 loss: 4.890 ppl: 132.992\n",
      "n_iter:26400 loss: 4.709 ppl: 110.933\n",
      "n_iter:26600 loss: 4.757 ppl: 116.442\n",
      "n_iter:26800 loss: 4.945 ppl: 140.421\n",
      "n_iter:27000 loss: 4.889 ppl: 132.776\n",
      "n_iter:27200 loss: 4.939 ppl: 139.601\n",
      "n_iter:27400 loss: 4.941 ppl: 139.931\n",
      "n_iter:27600 loss: 5.078 ppl: 160.530\n",
      "n_iter:27800 loss: 4.831 ppl: 125.306\n",
      "n_iter:28000 loss: 5.053 ppl: 156.424\n",
      "n_iter:28200 loss: 5.017 ppl: 150.984\n",
      "n_iter:28400 loss: 4.959 ppl: 142.407\n",
      "n_iter:28600 loss: 4.775 ppl: 118.518\n",
      "n_iter:28800 loss: 4.930 ppl: 138.349\n",
      "n_iter:29000 loss: 4.967 ppl: 143.557\n",
      "### find best mode ### 171.62226574660582\n",
      "validation vloss: 5.145 vppl: 171.622, best ppl: 171.622\n",
      "[4/50] epochs training...\n",
      "n_iter:29200 loss: 5.059 ppl: 157.356\n",
      "n_iter:29400 loss: 4.966 ppl: 143.439\n",
      "n_iter:29600 loss: 5.110 ppl: 165.588\n",
      "n_iter:29800 loss: 4.981 ppl: 145.658\n",
      "n_iter:30000 loss: 4.953 ppl: 141.603\n",
      "n_iter:30200 loss: 4.998 ppl: 148.153\n",
      "n_iter:30400 loss: 4.744 ppl: 114.907\n",
      "n_iter:30600 loss: 4.872 ppl: 130.625\n",
      "n_iter:30800 loss: 4.892 ppl: 133.183\n",
      "n_iter:31000 loss: 4.855 ppl: 128.360\n",
      "n_iter:31200 loss: 4.741 ppl: 114.591\n",
      "n_iter:31400 loss: 4.813 ppl: 123.109\n",
      "n_iter:31600 loss: 4.832 ppl: 125.401\n",
      "n_iter:31800 loss: 4.896 ppl: 133.774\n",
      "n_iter:32000 loss: 4.912 ppl: 135.929\n",
      "n_iter:32200 loss: 4.743 ppl: 114.743\n",
      "n_iter:32400 loss: 4.832 ppl: 125.464\n",
      "n_iter:32600 loss: 4.966 ppl: 143.477\n",
      "n_iter:32800 loss: 5.002 ppl: 148.708\n",
      "n_iter:33000 loss: 4.716 ppl: 111.714\n",
      "n_iter:33200 loss: 4.804 ppl: 122.051\n",
      "n_iter:33400 loss: 4.853 ppl: 128.142\n",
      "n_iter:33600 loss: 4.634 ppl: 102.890\n",
      "n_iter:33800 loss: 4.665 ppl: 106.163\n",
      "n_iter:34000 loss: 4.828 ppl: 125.023\n",
      "n_iter:34200 loss: 4.797 ppl: 121.088\n",
      "n_iter:34400 loss: 4.818 ppl: 123.719\n",
      "n_iter:34600 loss: 4.933 ppl: 138.758\n",
      "n_iter:34800 loss: 4.930 ppl: 138.410\n",
      "n_iter:35000 loss: 4.791 ppl: 120.381\n",
      "n_iter:35200 loss: 4.853 ppl: 128.074\n",
      "n_iter:35400 loss: 4.998 ppl: 148.142\n",
      "n_iter:35600 loss: 4.900 ppl: 134.281\n",
      "n_iter:35800 loss: 4.685 ppl: 108.279\n",
      "n_iter:36000 loss: 4.902 ppl: 134.545\n",
      "n_iter:36200 loss: 4.779 ppl: 118.998\n",
      "### find best mode ### 169.0446032192009\n",
      "validation vloss: 5.130 vppl: 169.045, best ppl: 169.045\n",
      "[5/50] epochs training...\n",
      "n_iter:36400 loss: 5.027 ppl: 152.400\n",
      "n_iter:36600 loss: 4.850 ppl: 127.753\n",
      "n_iter:36800 loss: 5.014 ppl: 150.520\n",
      "n_iter:37000 loss: 4.980 ppl: 145.474\n",
      "n_iter:37200 loss: 4.895 ppl: 133.667\n",
      "n_iter:37400 loss: 4.885 ppl: 132.298\n",
      "n_iter:37600 loss: 4.680 ppl: 107.806\n",
      "n_iter:37800 loss: 4.877 ppl: 131.190\n",
      "n_iter:38000 loss: 4.747 ppl: 115.287\n",
      "n_iter:38200 loss: 4.794 ppl: 120.762\n",
      "n_iter:38400 loss: 4.777 ppl: 118.764\n",
      "n_iter:38600 loss: 4.640 ppl: 103.546\n",
      "n_iter:38800 loss: 4.749 ppl: 115.493\n",
      "n_iter:39000 loss: 4.789 ppl: 120.167\n",
      "n_iter:39200 loss: 4.859 ppl: 128.833\n",
      "n_iter:39400 loss: 4.829 ppl: 125.055\n",
      "n_iter:39600 loss: 4.604 ppl: 99.902\n",
      "n_iter:39800 loss: 4.871 ppl: 130.392\n",
      "n_iter:40000 loss: 4.922 ppl: 137.306\n",
      "n_iter:40200 loss: 4.722 ppl: 112.396\n",
      "n_iter:40400 loss: 4.623 ppl: 101.763\n",
      "n_iter:40600 loss: 4.860 ppl: 129.053\n",
      "n_iter:40800 loss: 4.646 ppl: 104.219\n",
      "n_iter:41000 loss: 4.555 ppl: 95.143\n",
      "n_iter:41200 loss: 4.654 ppl: 105.006\n",
      "n_iter:41400 loss: 4.709 ppl: 110.912\n",
      "n_iter:41600 loss: 4.773 ppl: 118.231\n",
      "n_iter:41800 loss: 4.870 ppl: 130.341\n",
      "n_iter:42000 loss: 4.752 ppl: 115.799\n",
      "n_iter:42200 loss: 4.811 ppl: 122.900\n",
      "n_iter:42400 loss: 4.759 ppl: 116.606\n",
      "n_iter:42600 loss: 4.940 ppl: 139.800\n",
      "n_iter:42800 loss: 4.820 ppl: 123.924\n",
      "n_iter:43000 loss: 4.685 ppl: 108.346\n",
      "n_iter:43200 loss: 4.753 ppl: 115.926\n",
      "n_iter:43400 loss: 4.674 ppl: 107.162\n",
      "### find best mode ### 166.49159925966416\n",
      "validation vloss: 5.115 vppl: 166.492, best ppl: 166.492\n",
      "[6/50] epochs training...\n",
      "n_iter:43600 loss: 4.941 ppl: 139.845\n",
      "n_iter:43800 loss: 4.823 ppl: 124.285\n",
      "n_iter:44000 loss: 4.906 ppl: 135.097\n",
      "n_iter:44200 loss: 4.934 ppl: 138.968\n",
      "n_iter:44400 loss: 4.831 ppl: 125.295\n",
      "n_iter:44600 loss: 4.792 ppl: 120.578\n",
      "n_iter:44800 loss: 4.779 ppl: 118.979\n",
      "n_iter:45000 loss: 4.704 ppl: 110.415\n",
      "n_iter:45200 loss: 4.715 ppl: 111.587\n",
      "n_iter:45400 loss: 4.740 ppl: 114.429\n",
      "n_iter:45600 loss: 4.689 ppl: 108.740\n",
      "n_iter:45800 loss: 4.615 ppl: 101.032\n",
      "n_iter:46000 loss: 4.696 ppl: 109.554\n",
      "n_iter:46200 loss: 4.697 ppl: 109.633\n",
      "n_iter:46400 loss: 4.736 ppl: 113.965\n",
      "n_iter:46600 loss: 4.867 ppl: 129.905\n",
      "n_iter:46800 loss: 4.489 ppl: 89.025\n",
      "n_iter:47000 loss: 4.812 ppl: 122.982\n",
      "n_iter:47200 loss: 4.811 ppl: 122.859\n",
      "n_iter:47400 loss: 4.758 ppl: 116.483\n",
      "n_iter:47600 loss: 4.508 ppl: 90.784\n",
      "n_iter:47800 loss: 4.824 ppl: 124.417\n",
      "n_iter:48000 loss: 4.671 ppl: 106.823\n",
      "n_iter:48200 loss: 4.461 ppl: 86.594\n",
      "n_iter:48400 loss: 4.559 ppl: 95.464\n",
      "n_iter:48600 loss: 4.720 ppl: 112.201\n",
      "n_iter:48800 loss: 4.677 ppl: 107.446\n",
      "n_iter:49000 loss: 4.708 ppl: 110.874\n",
      "n_iter:49200 loss: 4.715 ppl: 111.582\n",
      "n_iter:49400 loss: 4.835 ppl: 125.809\n",
      "n_iter:49600 loss: 4.609 ppl: 100.390\n",
      "n_iter:49800 loss: 4.879 ppl: 131.435\n",
      "n_iter:50000 loss: 4.763 ppl: 117.060\n",
      "n_iter:50200 loss: 4.737 ppl: 114.146\n",
      "n_iter:50400 loss: 4.553 ppl: 94.957\n",
      "n_iter:50600 loss: 4.672 ppl: 106.925\n",
      "n_iter:50800 loss: 4.765 ppl: 117.341\n",
      "### find best mode ### 164.76047392330082\n",
      "validation vloss: 5.104 vppl: 164.760, best ppl: 164.760\n",
      "[7/50] epochs training...\n",
      "n_iter:51000 loss: 4.865 ppl: 129.716\n",
      "n_iter:51200 loss: 4.773 ppl: 118.275\n",
      "n_iter:51400 loss: 4.907 ppl: 135.222\n",
      "n_iter:51600 loss: 4.781 ppl: 119.275\n",
      "n_iter:51800 loss: 4.766 ppl: 117.463\n",
      "n_iter:52000 loss: 4.808 ppl: 122.460\n",
      "n_iter:52200 loss: 4.548 ppl: 94.452\n",
      "n_iter:52400 loss: 4.692 ppl: 109.062\n",
      "n_iter:52600 loss: 4.708 ppl: 110.846\n",
      "n_iter:52800 loss: 4.659 ppl: 105.550\n",
      "n_iter:53000 loss: 4.539 ppl: 93.626\n",
      "n_iter:53200 loss: 4.633 ppl: 102.824\n",
      "n_iter:53400 loss: 4.647 ppl: 104.278\n",
      "n_iter:53600 loss: 4.692 ppl: 109.097\n",
      "n_iter:53800 loss: 4.729 ppl: 113.228\n",
      "n_iter:54000 loss: 4.543 ppl: 93.932\n",
      "n_iter:54200 loss: 4.663 ppl: 105.994\n",
      "n_iter:54400 loss: 4.783 ppl: 119.464\n",
      "n_iter:54600 loss: 4.801 ppl: 121.599\n",
      "n_iter:54800 loss: 4.495 ppl: 89.540\n",
      "n_iter:55000 loss: 4.646 ppl: 104.152\n",
      "n_iter:55200 loss: 4.636 ppl: 103.160\n",
      "n_iter:55400 loss: 4.461 ppl: 86.534\n",
      "n_iter:55600 loss: 4.496 ppl: 89.654\n",
      "n_iter:55800 loss: 4.634 ppl: 102.915\n",
      "n_iter:56000 loss: 4.602 ppl: 99.715\n",
      "n_iter:56200 loss: 4.636 ppl: 103.131\n",
      "n_iter:56400 loss: 4.706 ppl: 110.637\n",
      "n_iter:56600 loss: 4.765 ppl: 117.289\n",
      "n_iter:56800 loss: 4.600 ppl: 99.498\n",
      "n_iter:57000 loss: 4.715 ppl: 111.576\n",
      "n_iter:57200 loss: 4.790 ppl: 120.278\n",
      "n_iter:57400 loss: 4.711 ppl: 111.149\n",
      "n_iter:57600 loss: 4.472 ppl: 87.572\n",
      "n_iter:57800 loss: 4.720 ppl: 112.187\n",
      "n_iter:58000 loss: 4.609 ppl: 100.390\n",
      "### find best mode ### 164.25633934054218\n",
      "validation vloss: 5.101 vppl: 164.256, best ppl: 164.256\n",
      "[8/50] epochs training...\n",
      "n_iter:58200 loss: 4.837 ppl: 126.064\n",
      "n_iter:58400 loss: 4.690 ppl: 108.891\n",
      "n_iter:58600 loss: 4.852 ppl: 128.005\n",
      "n_iter:58800 loss: 4.804 ppl: 121.962\n",
      "n_iter:59000 loss: 4.723 ppl: 112.541\n",
      "n_iter:59200 loss: 4.726 ppl: 112.866\n",
      "n_iter:59400 loss: 4.508 ppl: 90.695\n",
      "n_iter:59600 loss: 4.700 ppl: 109.907\n",
      "n_iter:59800 loss: 4.597 ppl: 99.147\n",
      "n_iter:60000 loss: 4.644 ppl: 103.989\n",
      "n_iter:60200 loss: 4.608 ppl: 100.309\n",
      "n_iter:60400 loss: 4.463 ppl: 86.715\n",
      "n_iter:60600 loss: 4.612 ppl: 100.689\n",
      "n_iter:60800 loss: 4.620 ppl: 101.539\n",
      "n_iter:61000 loss: 4.693 ppl: 109.201\n",
      "n_iter:61200 loss: 4.630 ppl: 102.514\n",
      "n_iter:61400 loss: 4.490 ppl: 89.108\n",
      "n_iter:61600 loss: 4.701 ppl: 110.078\n",
      "n_iter:61800 loss: 4.775 ppl: 118.540\n",
      "n_iter:62000 loss: 4.550 ppl: 94.641\n",
      "n_iter:62200 loss: 4.474 ppl: 87.683\n",
      "n_iter:62400 loss: 4.680 ppl: 107.726\n",
      "n_iter:62600 loss: 4.474 ppl: 87.733\n",
      "n_iter:62800 loss: 4.385 ppl: 80.219\n",
      "n_iter:63000 loss: 4.509 ppl: 90.874\n",
      "n_iter:63200 loss: 4.561 ppl: 95.662\n",
      "n_iter:63400 loss: 4.598 ppl: 99.304\n",
      "n_iter:63600 loss: 4.700 ppl: 109.924\n",
      "n_iter:63800 loss: 4.601 ppl: 99.567\n",
      "n_iter:64000 loss: 4.637 ppl: 103.256\n",
      "n_iter:64200 loss: 4.601 ppl: 99.560\n",
      "n_iter:64400 loss: 4.789 ppl: 120.175\n",
      "n_iter:64600 loss: 4.649 ppl: 104.522\n",
      "n_iter:64800 loss: 4.512 ppl: 91.092\n",
      "n_iter:65000 loss: 4.599 ppl: 99.392\n",
      "n_iter:65200 loss: 4.533 ppl: 93.024\n",
      "### find best mode ### 162.02703628104126\n",
      "validation vloss: 5.088 vppl: 162.027, best ppl: 162.027\n",
      "[9/50] epochs training...\n",
      "n_iter:65400 loss: 4.780 ppl: 119.093\n",
      "n_iter:65600 loss: 4.668 ppl: 106.530\n",
      "n_iter:65800 loss: 4.786 ppl: 119.846\n",
      "n_iter:66000 loss: 4.792 ppl: 120.510\n",
      "n_iter:66200 loss: 4.686 ppl: 108.408\n",
      "n_iter:66400 loss: 4.653 ppl: 104.909\n",
      "n_iter:66600 loss: 4.595 ppl: 98.996\n",
      "n_iter:66800 loss: 4.579 ppl: 97.376\n",
      "n_iter:67000 loss: 4.571 ppl: 96.646\n",
      "n_iter:67200 loss: 4.600 ppl: 99.512\n",
      "n_iter:67400 loss: 4.551 ppl: 94.694\n",
      "n_iter:67600 loss: 4.450 ppl: 85.601\n",
      "n_iter:67800 loss: 4.549 ppl: 94.496\n",
      "n_iter:68000 loss: 4.536 ppl: 93.325\n",
      "n_iter:68200 loss: 4.604 ppl: 99.854\n",
      "n_iter:68400 loss: 4.724 ppl: 112.634\n",
      "n_iter:68600 loss: 4.350 ppl: 77.459\n",
      "n_iter:68800 loss: 4.653 ppl: 104.896\n",
      "n_iter:69000 loss: 4.684 ppl: 108.219\n",
      "n_iter:69200 loss: 4.612 ppl: 100.671\n",
      "n_iter:69400 loss: 4.371 ppl: 79.113\n",
      "n_iter:69600 loss: 4.672 ppl: 106.924\n",
      "n_iter:69800 loss: 4.507 ppl: 90.645\n",
      "n_iter:70000 loss: 4.325 ppl: 75.549\n",
      "n_iter:70200 loss: 4.408 ppl: 82.129\n",
      "n_iter:70400 loss: 4.588 ppl: 98.308\n",
      "n_iter:70600 loss: 4.502 ppl: 90.209\n",
      "n_iter:70800 loss: 4.577 ppl: 97.174\n",
      "n_iter:71000 loss: 4.590 ppl: 98.496\n",
      "n_iter:71200 loss: 4.676 ppl: 107.375\n",
      "n_iter:71400 loss: 4.511 ppl: 90.970\n",
      "n_iter:71600 loss: 4.743 ppl: 114.751\n",
      "n_iter:71800 loss: 4.617 ppl: 101.144\n",
      "n_iter:72000 loss: 4.573 ppl: 96.795\n",
      "n_iter:72200 loss: 4.459 ppl: 86.389\n",
      "n_iter:72400 loss: 4.508 ppl: 90.776\n",
      "n_iter:72600 loss: 4.641 ppl: 103.620\n",
      "### find best mode ### 160.8503659154049\n",
      "validation vloss: 5.080 vppl: 160.850, best ppl: 160.850\n",
      "[10/50] epochs training...\n",
      "n_iter:72800 loss: 4.726 ppl: 112.793\n",
      "n_iter:73000 loss: 4.667 ppl: 106.402\n",
      "n_iter:73200 loss: 4.761 ppl: 116.846\n",
      "n_iter:73400 loss: 4.672 ppl: 106.943\n",
      "n_iter:73600 loss: 4.633 ppl: 102.777\n",
      "n_iter:73800 loss: 4.687 ppl: 108.499\n",
      "n_iter:74000 loss: 4.407 ppl: 82.028\n",
      "n_iter:74200 loss: 4.589 ppl: 98.368\n",
      "n_iter:74400 loss: 4.564 ppl: 95.981\n",
      "n_iter:74600 loss: 4.523 ppl: 92.075\n",
      "n_iter:74800 loss: 4.441 ppl: 84.858\n",
      "n_iter:75000 loss: 4.499 ppl: 89.902\n",
      "n_iter:75200 loss: 4.508 ppl: 90.711\n",
      "n_iter:75400 loss: 4.570 ppl: 96.541\n",
      "n_iter:75600 loss: 4.616 ppl: 101.104\n",
      "n_iter:75800 loss: 4.371 ppl: 79.132\n",
      "n_iter:76000 loss: 4.571 ppl: 96.667\n",
      "n_iter:76200 loss: 4.655 ppl: 105.129\n",
      "n_iter:76400 loss: 4.671 ppl: 106.812\n",
      "n_iter:76600 loss: 4.332 ppl: 76.076\n",
      "n_iter:76800 loss: 4.533 ppl: 93.081\n",
      "n_iter:77000 loss: 4.491 ppl: 89.209\n",
      "n_iter:77200 loss: 4.353 ppl: 77.733\n",
      "n_iter:77400 loss: 4.347 ppl: 77.267\n",
      "n_iter:77600 loss: 4.523 ppl: 92.134\n",
      "n_iter:77800 loss: 4.459 ppl: 86.368\n",
      "n_iter:78000 loss: 4.528 ppl: 92.602\n",
      "n_iter:78200 loss: 4.558 ppl: 95.421\n",
      "n_iter:78400 loss: 4.653 ppl: 104.940\n",
      "n_iter:78600 loss: 4.456 ppl: 86.138\n",
      "n_iter:78800 loss: 4.607 ppl: 100.183\n",
      "n_iter:79000 loss: 4.647 ppl: 104.268\n",
      "n_iter:79200 loss: 4.589 ppl: 98.432\n",
      "n_iter:79400 loss: 4.318 ppl: 75.049\n",
      "n_iter:79600 loss: 4.578 ppl: 97.326\n",
      "n_iter:79800 loss: 4.508 ppl: 90.746\n",
      "### find best mode ### 160.83451550582464\n",
      "validation vloss: 5.080 vppl: 160.835, best ppl: 160.835\n",
      "[11/50] epochs training...\n",
      "n_iter:80000 loss: 4.695 ppl: 109.419\n",
      "n_iter:80200 loss: 4.610 ppl: 100.532\n",
      "n_iter:80400 loss: 4.736 ppl: 114.030\n",
      "n_iter:80600 loss: 4.678 ppl: 107.595\n",
      "n_iter:80800 loss: 4.626 ppl: 102.156\n",
      "n_iter:81000 loss: 4.624 ppl: 101.908\n",
      "n_iter:81200 loss: 4.403 ppl: 81.719\n",
      "n_iter:81400 loss: 4.549 ppl: 94.497\n",
      "n_iter:81600 loss: 4.494 ppl: 89.510\n",
      "n_iter:81800 loss: 4.521 ppl: 91.956\n",
      "n_iter:82000 loss: 4.487 ppl: 88.895\n",
      "n_iter:82200 loss: 4.361 ppl: 78.335\n",
      "n_iter:82400 loss: 4.500 ppl: 90.013\n",
      "n_iter:82600 loss: 4.504 ppl: 90.333\n",
      "n_iter:82800 loss: 4.585 ppl: 98.011\n",
      "n_iter:83000 loss: 4.453 ppl: 85.898\n",
      "n_iter:83200 loss: 4.399 ppl: 81.354\n",
      "n_iter:83400 loss: 4.622 ppl: 101.747\n",
      "n_iter:83600 loss: 4.644 ppl: 103.992\n",
      "n_iter:83800 loss: 4.423 ppl: 83.359\n",
      "n_iter:84000 loss: 4.379 ppl: 79.775\n",
      "n_iter:84200 loss: 4.565 ppl: 96.039\n",
      "n_iter:84400 loss: 4.343 ppl: 76.960\n",
      "n_iter:84600 loss: 4.276 ppl: 71.977\n",
      "n_iter:84800 loss: 4.418 ppl: 82.935\n",
      "n_iter:85000 loss: 4.457 ppl: 86.233\n",
      "n_iter:85200 loss: 4.476 ppl: 87.909\n",
      "n_iter:85400 loss: 4.597 ppl: 99.148\n",
      "n_iter:85600 loss: 4.488 ppl: 88.914\n",
      "n_iter:85800 loss: 4.504 ppl: 90.369\n",
      "n_iter:86000 loss: 4.491 ppl: 89.196\n",
      "n_iter:86200 loss: 4.670 ppl: 106.690\n",
      "n_iter:86400 loss: 4.544 ppl: 94.062\n",
      "n_iter:86600 loss: 4.372 ppl: 79.184\n",
      "n_iter:86800 loss: 4.497 ppl: 89.785\n",
      "n_iter:87000 loss: 4.425 ppl: 83.487\n",
      "### find best mode ### 158.28268691290657\n",
      "validation vloss: 5.064 vppl: 158.283, best ppl: 158.283\n",
      "[12/50] epochs training...\n",
      "n_iter:87200 loss: 4.671 ppl: 106.807\n",
      "n_iter:87400 loss: 4.547 ppl: 94.321\n",
      "n_iter:87600 loss: 4.678 ppl: 107.513\n",
      "n_iter:87800 loss: 4.703 ppl: 110.315\n",
      "n_iter:88000 loss: 4.577 ppl: 97.228\n",
      "n_iter:88200 loss: 4.562 ppl: 95.792\n",
      "n_iter:88400 loss: 4.467 ppl: 87.092\n",
      "n_iter:88600 loss: 4.490 ppl: 89.149\n",
      "n_iter:88800 loss: 4.478 ppl: 88.066\n",
      "n_iter:89000 loss: 4.488 ppl: 88.947\n",
      "n_iter:89200 loss: 4.456 ppl: 86.124\n",
      "n_iter:89400 loss: 4.352 ppl: 77.664\n",
      "n_iter:89600 loss: 4.435 ppl: 84.348\n",
      "n_iter:89800 loss: 4.446 ppl: 85.282\n",
      "n_iter:90000 loss: 4.511 ppl: 91.036\n",
      "n_iter:90200 loss: 4.616 ppl: 101.113\n",
      "n_iter:90400 loss: 4.253 ppl: 70.292\n",
      "n_iter:90600 loss: 4.544 ppl: 94.020\n",
      "n_iter:90800 loss: 4.599 ppl: 99.421\n",
      "n_iter:91000 loss: 4.468 ppl: 87.220\n",
      "n_iter:91200 loss: 4.267 ppl: 71.318\n",
      "n_iter:91400 loss: 4.563 ppl: 95.888\n",
      "n_iter:91600 loss: 4.415 ppl: 82.713\n",
      "n_iter:91800 loss: 4.202 ppl: 66.844\n",
      "n_iter:92000 loss: 4.326 ppl: 75.637\n",
      "n_iter:92200 loss: 4.453 ppl: 85.867\n",
      "n_iter:92400 loss: 4.431 ppl: 84.023\n",
      "n_iter:92600 loss: 4.499 ppl: 89.941\n",
      "n_iter:92800 loss: 4.469 ppl: 87.237\n",
      "n_iter:93000 loss: 4.546 ppl: 94.277\n",
      "n_iter:93200 loss: 4.425 ppl: 83.554\n",
      "n_iter:93400 loss: 4.646 ppl: 104.117\n",
      "n_iter:93600 loss: 4.500 ppl: 89.989\n",
      "n_iter:93800 loss: 4.419 ppl: 82.974\n",
      "n_iter:94000 loss: 4.395 ppl: 81.052\n",
      "n_iter:94200 loss: 4.390 ppl: 80.663\n",
      "n_iter:94400 loss: 4.571 ppl: 96.622\n",
      "### find best mode ### 157.3507006967104\n",
      "validation vloss: 5.058 vppl: 157.351, best ppl: 157.351\n",
      "[13/50] epochs training...\n",
      "n_iter:94600 loss: 4.629 ppl: 102.426\n",
      "n_iter:94800 loss: 4.569 ppl: 96.424\n",
      "n_iter:95000 loss: 4.662 ppl: 105.900\n",
      "n_iter:95200 loss: 4.585 ppl: 97.959\n",
      "n_iter:95400 loss: 4.532 ppl: 92.931\n",
      "n_iter:95600 loss: 4.572 ppl: 96.786\n",
      "n_iter:95800 loss: 4.331 ppl: 75.998\n",
      "n_iter:96000 loss: 4.484 ppl: 88.626\n",
      "n_iter:96200 loss: 4.443 ppl: 85.016\n",
      "n_iter:96400 loss: 4.405 ppl: 81.867\n",
      "n_iter:96600 loss: 4.366 ppl: 78.738\n",
      "n_iter:96800 loss: 4.404 ppl: 81.811\n",
      "n_iter:97000 loss: 4.434 ppl: 84.230\n",
      "n_iter:97200 loss: 4.462 ppl: 86.649\n",
      "n_iter:97400 loss: 4.544 ppl: 94.072\n",
      "n_iter:97600 loss: 4.247 ppl: 69.929\n",
      "n_iter:97800 loss: 4.488 ppl: 88.933\n",
      "n_iter:98000 loss: 4.571 ppl: 96.639\n",
      "n_iter:98200 loss: 4.548 ppl: 94.432\n",
      "n_iter:98400 loss: 4.236 ppl: 69.100\n",
      "n_iter:98600 loss: 4.487 ppl: 88.847\n",
      "n_iter:98800 loss: 4.367 ppl: 78.804\n",
      "n_iter:99000 loss: 4.248 ppl: 69.994\n",
      "n_iter:99200 loss: 4.271 ppl: 71.561\n",
      "n_iter:99400 loss: 4.436 ppl: 84.404\n",
      "n_iter:99600 loss: 4.339 ppl: 76.652\n",
      "n_iter:99800 loss: 4.449 ppl: 85.570\n",
      "n_iter:100000 loss: 4.430 ppl: 83.952\n",
      "n_iter:100200 loss: 4.562 ppl: 95.742\n",
      "n_iter:100400 loss: 4.358 ppl: 78.139\n",
      "n_iter:100600 loss: 4.520 ppl: 91.806\n",
      "n_iter:100800 loss: 4.546 ppl: 94.285\n",
      "n_iter:101000 loss: 4.509 ppl: 90.794\n",
      "n_iter:101200 loss: 4.222 ppl: 68.162\n",
      "n_iter:101400 loss: 4.473 ppl: 87.581\n",
      "n_iter:101600 loss: 4.454 ppl: 85.931\n",
      "### find best mode ### 156.7533231233186\n",
      "validation vloss: 5.055 vppl: 156.753, best ppl: 156.753\n",
      "[14/50] epochs training...\n",
      "n_iter:101800 loss: 4.581 ppl: 97.647\n",
      "n_iter:102000 loss: 4.531 ppl: 92.870\n",
      "n_iter:102200 loss: 4.641 ppl: 103.625\n",
      "n_iter:102400 loss: 4.575 ppl: 97.003\n",
      "n_iter:102600 loss: 4.540 ppl: 93.694\n",
      "n_iter:102800 loss: 4.557 ppl: 95.270\n",
      "n_iter:103000 loss: 4.298 ppl: 73.541\n",
      "n_iter:103200 loss: 4.421 ppl: 83.208\n",
      "n_iter:103400 loss: 4.443 ppl: 85.016\n",
      "n_iter:103600 loss: 4.440 ppl: 84.777\n",
      "n_iter:103800 loss: 4.367 ppl: 78.813\n",
      "n_iter:104000 loss: 4.313 ppl: 74.694\n",
      "n_iter:104200 loss: 4.363 ppl: 78.463\n",
      "n_iter:104400 loss: 4.444 ppl: 85.101\n",
      "n_iter:104600 loss: 4.479 ppl: 88.148\n",
      "n_iter:104800 loss: 4.336 ppl: 76.402\n",
      "n_iter:105000 loss: 4.351 ppl: 77.563\n",
      "n_iter:105200 loss: 4.555 ppl: 95.146\n",
      "n_iter:105400 loss: 4.559 ppl: 95.525\n",
      "n_iter:105600 loss: 4.333 ppl: 76.188\n",
      "n_iter:105800 loss: 4.304 ppl: 73.968\n",
      "n_iter:106000 loss: 4.437 ppl: 84.533\n",
      "n_iter:106200 loss: 4.260 ppl: 70.817\n",
      "n_iter:106400 loss: 4.189 ppl: 65.969\n",
      "n_iter:106600 loss: 4.344 ppl: 76.983\n",
      "n_iter:106800 loss: 4.394 ppl: 80.929\n",
      "n_iter:107000 loss: 4.365 ppl: 78.610\n",
      "n_iter:107200 loss: 4.505 ppl: 90.430\n",
      "n_iter:107400 loss: 4.430 ppl: 83.954\n",
      "n_iter:107600 loss: 4.387 ppl: 80.374\n",
      "n_iter:107800 loss: 4.447 ppl: 85.352\n",
      "n_iter:108000 loss: 4.574 ppl: 96.972\n",
      "n_iter:108200 loss: 4.451 ppl: 85.752\n",
      "n_iter:108400 loss: 4.277 ppl: 72.055\n",
      "n_iter:108600 loss: 4.426 ppl: 83.614\n",
      "n_iter:108800 loss: 4.349 ppl: 77.363\n",
      "### find best mode ### 156.04800183243484\n",
      "validation vloss: 5.050 vppl: 156.048, best ppl: 156.048\n",
      "[15/50] epochs training...\n",
      "n_iter:109000 loss: 4.600 ppl: 99.460\n",
      "n_iter:109200 loss: 4.454 ppl: 85.937\n",
      "n_iter:109400 loss: 4.592 ppl: 98.673\n",
      "n_iter:109600 loss: 4.617 ppl: 101.235\n",
      "n_iter:109800 loss: 4.494 ppl: 89.506\n",
      "n_iter:110000 loss: 4.476 ppl: 87.892\n",
      "n_iter:110200 loss: 4.347 ppl: 77.208\n",
      "n_iter:110400 loss: 4.416 ppl: 82.747\n",
      "n_iter:110600 loss: 4.415 ppl: 82.695\n",
      "n_iter:110800 loss: 4.402 ppl: 81.605\n",
      "n_iter:111000 loss: 4.394 ppl: 80.952\n",
      "n_iter:111200 loss: 4.272 ppl: 71.644\n",
      "n_iter:111400 loss: 4.334 ppl: 76.228\n",
      "n_iter:111600 loss: 4.349 ppl: 77.414\n",
      "n_iter:111800 loss: 4.442 ppl: 84.904\n",
      "n_iter:112000 loss: 4.541 ppl: 93.740\n",
      "n_iter:112200 loss: 4.176 ppl: 65.135\n",
      "n_iter:112400 loss: 4.484 ppl: 88.587\n",
      "n_iter:112600 loss: 4.506 ppl: 90.571\n",
      "n_iter:112800 loss: 4.391 ppl: 80.691\n",
      "n_iter:113000 loss: 4.181 ppl: 65.410\n",
      "n_iter:113200 loss: 4.487 ppl: 88.831\n",
      "n_iter:113400 loss: 4.312 ppl: 74.621\n",
      "n_iter:113600 loss: 4.122 ppl: 61.682\n",
      "n_iter:113800 loss: 4.264 ppl: 71.105\n",
      "n_iter:114000 loss: 4.340 ppl: 76.724\n",
      "n_iter:114200 loss: 4.349 ppl: 77.390\n",
      "n_iter:114400 loss: 4.444 ppl: 85.086\n",
      "n_iter:114600 loss: 4.359 ppl: 78.173\n",
      "n_iter:114800 loss: 4.460 ppl: 86.491\n",
      "n_iter:115000 loss: 4.359 ppl: 78.147\n",
      "n_iter:115200 loss: 4.565 ppl: 96.090\n",
      "n_iter:115400 loss: 4.427 ppl: 83.660\n",
      "n_iter:115600 loss: 4.308 ppl: 74.296\n",
      "n_iter:115800 loss: 4.340 ppl: 76.705\n",
      "n_iter:116000 loss: 4.307 ppl: 74.202\n",
      "n_iter:116200 loss: 4.503 ppl: 90.290\n",
      "### find best mode ### 155.74096615048143\n",
      "validation vloss: 5.048 vppl: 155.741, best ppl: 155.741\n",
      "[16/50] epochs training...\n",
      "n_iter:116400 loss: 4.527 ppl: 92.457\n",
      "n_iter:116600 loss: 4.502 ppl: 90.236\n",
      "n_iter:116800 loss: 4.579 ppl: 97.464\n",
      "n_iter:117000 loss: 4.503 ppl: 90.301\n",
      "n_iter:117200 loss: 4.433 ppl: 84.217\n",
      "n_iter:117400 loss: 4.491 ppl: 89.198\n",
      "n_iter:117600 loss: 4.268 ppl: 71.385\n",
      "n_iter:117800 loss: 4.397 ppl: 81.215\n",
      "n_iter:118000 loss: 4.375 ppl: 79.474\n",
      "n_iter:118200 loss: 4.323 ppl: 75.404\n",
      "n_iter:118400 loss: 4.279 ppl: 72.188\n",
      "n_iter:118600 loss: 4.332 ppl: 76.125\n",
      "n_iter:118800 loss: 4.344 ppl: 76.987\n",
      "n_iter:119000 loss: 4.389 ppl: 80.540\n",
      "n_iter:119200 loss: 4.485 ppl: 88.679\n",
      "n_iter:119400 loss: 4.175 ppl: 65.066\n",
      "n_iter:119600 loss: 4.409 ppl: 82.190\n",
      "n_iter:119800 loss: 4.503 ppl: 90.317\n",
      "n_iter:120000 loss: 4.453 ppl: 85.919\n",
      "n_iter:120200 loss: 4.169 ppl: 64.661\n",
      "n_iter:120400 loss: 4.422 ppl: 83.222\n",
      "n_iter:120600 loss: 4.306 ppl: 74.111\n",
      "n_iter:120800 loss: 4.183 ppl: 65.555\n",
      "n_iter:121000 loss: 4.189 ppl: 65.989\n",
      "n_iter:121200 loss: 4.361 ppl: 78.326\n",
      "n_iter:121400 loss: 4.268 ppl: 71.363\n",
      "n_iter:121600 loss: 4.375 ppl: 79.446\n",
      "n_iter:121800 loss: 4.345 ppl: 77.093\n",
      "n_iter:122000 loss: 4.508 ppl: 90.740\n",
      "n_iter:122200 loss: 4.270 ppl: 71.532\n",
      "n_iter:122400 loss: 4.474 ppl: 87.712\n",
      "n_iter:122600 loss: 4.464 ppl: 86.818\n",
      "n_iter:122800 loss: 4.408 ppl: 82.091\n",
      "n_iter:123000 loss: 4.174 ppl: 65.001\n",
      "n_iter:123200 loss: 4.394 ppl: 80.952\n",
      "n_iter:123400 loss: 4.389 ppl: 80.563\n",
      "### find best mode ### 155.090483762931\n",
      "validation vloss: 5.044 vppl: 155.090, best ppl: 155.090\n",
      "[17/50] epochs training...\n",
      "n_iter:123600 loss: 4.504 ppl: 90.416\n",
      "n_iter:123800 loss: 4.458 ppl: 86.331\n",
      "n_iter:124000 loss: 4.564 ppl: 95.957\n",
      "n_iter:124200 loss: 4.490 ppl: 89.110\n",
      "n_iter:124400 loss: 4.451 ppl: 85.742\n",
      "n_iter:124600 loss: 4.504 ppl: 90.407\n",
      "n_iter:124800 loss: 4.202 ppl: 66.810\n",
      "n_iter:125000 loss: 4.344 ppl: 77.019\n",
      "n_iter:125200 loss: 4.392 ppl: 80.768\n",
      "n_iter:125400 loss: 4.345 ppl: 77.093\n",
      "n_iter:125600 loss: 4.283 ppl: 72.464\n",
      "n_iter:125800 loss: 4.278 ppl: 72.060\n",
      "n_iter:126000 loss: 4.291 ppl: 73.006\n",
      "n_iter:126200 loss: 4.377 ppl: 79.606\n",
      "n_iter:126400 loss: 4.408 ppl: 82.079\n",
      "n_iter:126600 loss: 4.247 ppl: 69.929\n",
      "n_iter:126800 loss: 4.303 ppl: 73.921\n",
      "n_iter:127000 loss: 4.481 ppl: 88.308\n",
      "n_iter:127200 loss: 4.487 ppl: 88.883\n",
      "n_iter:127400 loss: 4.248 ppl: 69.931\n",
      "n_iter:127600 loss: 4.249 ppl: 70.038\n",
      "n_iter:127800 loss: 4.349 ppl: 77.407\n",
      "n_iter:128000 loss: 4.182 ppl: 65.491\n",
      "n_iter:128200 loss: 4.139 ppl: 62.757\n",
      "n_iter:128400 loss: 4.293 ppl: 73.167\n",
      "n_iter:128600 loss: 4.306 ppl: 74.119\n",
      "n_iter:128800 loss: 4.277 ppl: 72.026\n",
      "n_iter:129000 loss: 4.408 ppl: 82.096\n",
      "n_iter:129200 loss: 4.392 ppl: 80.784\n",
      "n_iter:129400 loss: 4.309 ppl: 74.345\n",
      "n_iter:129600 loss: 4.359 ppl: 78.189\n",
      "n_iter:129800 loss: 4.510 ppl: 90.910\n",
      "n_iter:130000 loss: 4.366 ppl: 78.692\n",
      "n_iter:130200 loss: 4.212 ppl: 67.514\n",
      "n_iter:130400 loss: 4.362 ppl: 78.421\n",
      "n_iter:130600 loss: 4.274 ppl: 71.779\n",
      "### find best mode ### 154.7840630263876\n",
      "validation vloss: 5.042 vppl: 154.784, best ppl: 154.784\n",
      "[18/50] epochs training...\n",
      "n_iter:130800 loss: 4.533 ppl: 93.046\n",
      "n_iter:131000 loss: 4.377 ppl: 79.566\n",
      "n_iter:131200 loss: 4.542 ppl: 93.922\n",
      "n_iter:131400 loss: 4.538 ppl: 93.501\n",
      "n_iter:131600 loss: 4.448 ppl: 85.429\n",
      "n_iter:131800 loss: 4.405 ppl: 81.838\n",
      "n_iter:132000 loss: 4.268 ppl: 71.344\n",
      "n_iter:132200 loss: 4.385 ppl: 80.225\n",
      "n_iter:132400 loss: 4.323 ppl: 75.444\n",
      "n_iter:132600 loss: 4.333 ppl: 76.165\n",
      "n_iter:132800 loss: 4.309 ppl: 74.393\n",
      "n_iter:133000 loss: 4.202 ppl: 66.819\n",
      "n_iter:133200 loss: 4.264 ppl: 71.120\n",
      "n_iter:133400 loss: 4.305 ppl: 74.097\n",
      "n_iter:133600 loss: 4.386 ppl: 80.280\n",
      "n_iter:133800 loss: 4.458 ppl: 86.304\n",
      "n_iter:134000 loss: 4.096 ppl: 60.102\n",
      "n_iter:134200 loss: 4.429 ppl: 83.878\n",
      "n_iter:134400 loss: 4.457 ppl: 86.209\n",
      "n_iter:134600 loss: 4.308 ppl: 74.313\n",
      "n_iter:134800 loss: 4.123 ppl: 61.767\n",
      "n_iter:135000 loss: 4.402 ppl: 81.634\n",
      "n_iter:135200 loss: 4.241 ppl: 69.502\n",
      "n_iter:135400 loss: 4.081 ppl: 59.228\n",
      "n_iter:135600 loss: 4.194 ppl: 66.308\n",
      "n_iter:135800 loss: 4.271 ppl: 71.623\n",
      "n_iter:136000 loss: 4.288 ppl: 72.855\n",
      "n_iter:136200 loss: 4.396 ppl: 81.115\n",
      "n_iter:136400 loss: 4.277 ppl: 71.998\n",
      "n_iter:136600 loss: 4.401 ppl: 81.543\n",
      "n_iter:136800 loss: 4.289 ppl: 72.862\n",
      "n_iter:137000 loss: 4.503 ppl: 90.303\n",
      "n_iter:137200 loss: 4.368 ppl: 78.887\n",
      "n_iter:137400 loss: 4.241 ppl: 69.471\n",
      "n_iter:137600 loss: 4.284 ppl: 72.547\n",
      "n_iter:137800 loss: 4.241 ppl: 69.447\n",
      "### find best mode ### 154.56558705022584\n",
      "validation vloss: 5.041 vppl: 154.566, best ppl: 154.566\n",
      "[19/50] epochs training...\n",
      "n_iter:138000 loss: 4.458 ppl: 86.338\n",
      "n_iter:138200 loss: 4.459 ppl: 86.366\n",
      "n_iter:138400 loss: 4.456 ppl: 86.160\n",
      "n_iter:138600 loss: 4.518 ppl: 91.643\n",
      "n_iter:138800 loss: 4.437 ppl: 84.529\n",
      "n_iter:139000 loss: 4.380 ppl: 79.862\n",
      "n_iter:139200 loss: 4.422 ppl: 83.293\n",
      "n_iter:139400 loss: 4.236 ppl: 69.111\n",
      "n_iter:139600 loss: 4.316 ppl: 74.867\n",
      "n_iter:139800 loss: 4.316 ppl: 74.863\n",
      "n_iter:140000 loss: 4.289 ppl: 72.867\n",
      "n_iter:140200 loss: 4.207 ppl: 67.186\n",
      "n_iter:140400 loss: 4.268 ppl: 71.380\n",
      "n_iter:140600 loss: 4.277 ppl: 71.992\n",
      "n_iter:140800 loss: 4.336 ppl: 76.438\n",
      "n_iter:141000 loss: 4.414 ppl: 82.638\n",
      "n_iter:141200 loss: 4.099 ppl: 60.301\n",
      "n_iter:141400 loss: 4.365 ppl: 78.656\n",
      "n_iter:141600 loss: 4.448 ppl: 85.491\n",
      "n_iter:141800 loss: 4.366 ppl: 78.766\n",
      "n_iter:142000 loss: 4.093 ppl: 59.901\n",
      "n_iter:142200 loss: 4.382 ppl: 79.991\n",
      "n_iter:142400 loss: 4.246 ppl: 69.820\n",
      "n_iter:142600 loss: 4.108 ppl: 60.853\n",
      "n_iter:142800 loss: 4.134 ppl: 62.444\n",
      "n_iter:143000 loss: 4.311 ppl: 74.522\n",
      "n_iter:143200 loss: 4.212 ppl: 67.516\n",
      "n_iter:143400 loss: 4.315 ppl: 74.794\n",
      "n_iter:143600 loss: 4.291 ppl: 73.023\n",
      "n_iter:143800 loss: 4.467 ppl: 87.072\n",
      "n_iter:144000 loss: 4.205 ppl: 66.993\n",
      "n_iter:144200 loss: 4.433 ppl: 84.150\n",
      "n_iter:144400 loss: 4.393 ppl: 80.847\n",
      "n_iter:144600 loss: 4.319 ppl: 75.127\n",
      "n_iter:144800 loss: 4.133 ppl: 62.346\n",
      "n_iter:145000 loss: 4.301 ppl: 73.746\n",
      "n_iter:145200 loss: 4.346 ppl: 77.146\n",
      "validation vloss: 5.042 vppl: 154.834, best ppl: 154.566\n",
      "[20/50] epochs training...\n",
      "n_iter:145400 loss: 4.439 ppl: 84.732\n",
      "n_iter:145600 loss: 4.408 ppl: 82.099\n",
      "n_iter:145800 loss: 4.536 ppl: 93.343\n",
      "n_iter:146000 loss: 4.418 ppl: 82.943\n",
      "n_iter:146200 loss: 4.394 ppl: 80.974\n",
      "n_iter:146400 loss: 4.432 ppl: 84.078\n",
      "n_iter:146600 loss: 4.163 ppl: 64.263\n",
      "n_iter:146800 loss: 4.281 ppl: 72.293\n",
      "n_iter:147000 loss: 4.342 ppl: 76.854\n",
      "n_iter:147200 loss: 4.272 ppl: 71.688\n",
      "n_iter:147400 loss: 4.230 ppl: 68.737\n",
      "n_iter:147600 loss: 4.225 ppl: 68.353\n",
      "n_iter:147800 loss: 4.221 ppl: 68.105\n",
      "n_iter:148000 loss: 4.334 ppl: 76.278\n",
      "n_iter:148200 loss: 4.339 ppl: 76.630\n",
      "n_iter:148400 loss: 4.198 ppl: 66.549\n",
      "n_iter:148600 loss: 4.251 ppl: 70.199\n",
      "n_iter:148800 loss: 4.421 ppl: 83.153\n",
      "n_iter:149000 loss: 4.444 ppl: 85.154\n",
      "n_iter:149200 loss: 4.166 ppl: 64.476\n",
      "n_iter:149400 loss: 4.225 ppl: 68.361\n",
      "n_iter:149600 loss: 4.283 ppl: 72.485\n",
      "n_iter:149800 loss: 4.101 ppl: 60.428\n",
      "n_iter:150000 loss: 4.108 ppl: 60.854\n",
      "n_iter:150200 loss: 4.261 ppl: 70.855\n",
      "n_iter:150400 loss: 4.240 ppl: 69.376\n",
      "n_iter:150600 loss: 4.238 ppl: 69.270\n",
      "n_iter:150800 loss: 4.340 ppl: 76.688\n",
      "n_iter:151000 loss: 4.355 ppl: 77.856\n",
      "n_iter:151200 loss: 4.247 ppl: 69.896\n",
      "n_iter:151400 loss: 4.294 ppl: 73.241\n",
      "n_iter:151600 loss: 4.442 ppl: 84.960\n",
      "n_iter:151800 loss: 4.329 ppl: 75.837\n",
      "n_iter:152000 loss: 4.137 ppl: 62.638\n",
      "n_iter:152200 loss: 4.322 ppl: 75.374\n",
      "n_iter:152400 loss: 4.230 ppl: 68.702\n",
      "### find best mode ### 154.0008516723395\n",
      "validation vloss: 5.037 vppl: 154.001, best ppl: 154.001\n",
      "[21/50] epochs training...\n",
      "n_iter:152600 loss: 4.471 ppl: 87.461\n",
      "n_iter:152800 loss: 4.340 ppl: 76.718\n",
      "n_iter:153000 loss: 4.489 ppl: 89.053\n",
      "n_iter:153200 loss: 4.485 ppl: 88.665\n",
      "n_iter:153400 loss: 4.389 ppl: 80.530\n",
      "n_iter:153600 loss: 4.361 ppl: 78.341\n",
      "n_iter:153800 loss: 4.189 ppl: 65.970\n",
      "n_iter:154000 loss: 4.328 ppl: 75.823\n",
      "n_iter:154200 loss: 4.267 ppl: 71.297\n",
      "n_iter:154400 loss: 4.281 ppl: 72.303\n",
      "n_iter:154600 loss: 4.267 ppl: 71.317\n",
      "n_iter:154800 loss: 4.126 ppl: 61.901\n",
      "n_iter:155000 loss: 4.219 ppl: 67.969\n",
      "n_iter:155200 loss: 4.267 ppl: 71.324\n",
      "n_iter:155400 loss: 4.318 ppl: 75.071\n",
      "n_iter:155600 loss: 4.370 ppl: 79.030\n",
      "n_iter:155800 loss: 4.072 ppl: 58.701\n",
      "n_iter:156000 loss: 4.365 ppl: 78.632\n",
      "n_iter:156200 loss: 4.431 ppl: 84.012\n",
      "n_iter:156400 loss: 4.218 ppl: 67.918\n",
      "n_iter:156600 loss: 4.090 ppl: 59.735\n",
      "n_iter:156800 loss: 4.347 ppl: 77.223\n",
      "n_iter:157000 loss: 4.204 ppl: 66.976\n",
      "n_iter:157200 loss: 4.040 ppl: 56.803\n",
      "n_iter:157400 loss: 4.162 ppl: 64.229\n",
      "n_iter:157600 loss: 4.216 ppl: 67.787\n",
      "n_iter:157800 loss: 4.239 ppl: 69.321\n",
      "n_iter:158000 loss: 4.350 ppl: 77.468\n",
      "n_iter:158200 loss: 4.234 ppl: 68.964\n",
      "n_iter:158400 loss: 4.334 ppl: 76.226\n",
      "n_iter:158600 loss: 4.244 ppl: 69.685\n",
      "n_iter:158800 loss: 4.440 ppl: 84.753\n",
      "n_iter:159000 loss: 4.334 ppl: 76.239\n",
      "n_iter:159200 loss: 4.169 ppl: 64.640\n",
      "n_iter:159400 loss: 4.243 ppl: 69.594\n",
      "n_iter:159600 loss: 4.164 ppl: 64.359\n",
      "### find best mode ### 153.23571682202652\n",
      "validation vloss: 5.032 vppl: 153.236, best ppl: 153.236\n",
      "[22/50] epochs training...\n",
      "n_iter:159800 loss: 4.429 ppl: 83.854\n",
      "n_iter:160000 loss: 4.365 ppl: 78.640\n",
      "n_iter:160200 loss: 4.434 ppl: 84.244\n",
      "n_iter:160400 loss: 4.481 ppl: 88.304\n",
      "n_iter:160600 loss: 4.380 ppl: 79.863\n",
      "n_iter:160800 loss: 4.316 ppl: 74.905\n",
      "n_iter:161000 loss: 4.333 ppl: 76.202\n",
      "n_iter:161200 loss: 4.214 ppl: 67.624\n",
      "n_iter:161400 loss: 4.250 ppl: 70.097\n",
      "n_iter:161600 loss: 4.266 ppl: 71.243\n",
      "n_iter:161800 loss: 4.225 ppl: 68.398\n",
      "n_iter:162000 loss: 4.162 ppl: 64.224\n",
      "n_iter:162200 loss: 4.222 ppl: 68.162\n",
      "n_iter:162400 loss: 4.228 ppl: 68.605\n",
      "n_iter:162600 loss: 4.277 ppl: 72.042\n",
      "n_iter:162800 loss: 4.386 ppl: 80.331\n",
      "n_iter:163000 loss: 4.049 ppl: 57.351\n",
      "n_iter:163200 loss: 4.321 ppl: 75.239\n",
      "n_iter:163400 loss: 4.382 ppl: 80.006\n",
      "n_iter:163600 loss: 4.315 ppl: 74.826\n",
      "n_iter:163800 loss: 4.047 ppl: 57.216\n",
      "n_iter:164000 loss: 4.345 ppl: 77.109\n",
      "n_iter:164200 loss: 4.199 ppl: 66.602\n",
      "n_iter:164400 loss: 4.032 ppl: 56.398\n",
      "n_iter:164600 loss: 4.102 ppl: 60.474\n",
      "n_iter:164800 loss: 4.264 ppl: 71.127\n",
      "n_iter:165000 loss: 4.193 ppl: 66.219\n",
      "n_iter:165200 loss: 4.264 ppl: 71.104\n",
      "n_iter:165400 loss: 4.229 ppl: 68.670\n",
      "n_iter:165600 loss: 4.410 ppl: 82.276\n",
      "n_iter:165800 loss: 4.158 ppl: 63.946\n",
      "n_iter:166000 loss: 4.386 ppl: 80.305\n",
      "n_iter:166200 loss: 4.333 ppl: 76.152\n",
      "n_iter:166400 loss: 4.274 ppl: 71.796\n",
      "n_iter:166600 loss: 4.097 ppl: 60.171\n",
      "n_iter:166800 loss: 4.221 ppl: 68.092\n",
      "n_iter:167000 loss: 4.318 ppl: 75.020\n",
      "validation vloss: 5.033 vppl: 153.442, best ppl: 153.236\n",
      "[23/50] epochs training...\n",
      "n_iter:167200 loss: 4.387 ppl: 80.396\n",
      "n_iter:167400 loss: 4.370 ppl: 79.013\n",
      "n_iter:167600 loss: 4.472 ppl: 87.568\n",
      "n_iter:167800 loss: 4.369 ppl: 78.959\n",
      "n_iter:168000 loss: 4.335 ppl: 76.354\n",
      "n_iter:168200 loss: 4.385 ppl: 80.257\n",
      "n_iter:168400 loss: 4.124 ppl: 61.790\n",
      "n_iter:168600 loss: 4.241 ppl: 69.478\n",
      "n_iter:168800 loss: 4.300 ppl: 73.710\n",
      "n_iter:169000 loss: 4.240 ppl: 69.436\n",
      "n_iter:169200 loss: 4.149 ppl: 63.365\n",
      "n_iter:169400 loss: 4.172 ppl: 64.849\n",
      "n_iter:169600 loss: 4.186 ppl: 65.752\n",
      "n_iter:169800 loss: 4.288 ppl: 72.816\n",
      "n_iter:170000 loss: 4.266 ppl: 71.265\n",
      "n_iter:170200 loss: 4.142 ppl: 62.960\n",
      "n_iter:170400 loss: 4.234 ppl: 68.986\n",
      "n_iter:170600 loss: 4.368 ppl: 78.903\n",
      "n_iter:170800 loss: 4.394 ppl: 81.004\n",
      "n_iter:171000 loss: 4.095 ppl: 60.034\n",
      "n_iter:171200 loss: 4.185 ppl: 65.712\n",
      "n_iter:171400 loss: 4.262 ppl: 70.974\n",
      "n_iter:171600 loss: 4.055 ppl: 57.703\n",
      "n_iter:171800 loss: 4.074 ppl: 58.766\n",
      "n_iter:172000 loss: 4.223 ppl: 68.213\n",
      "n_iter:172200 loss: 4.177 ppl: 65.193\n",
      "n_iter:172400 loss: 4.195 ppl: 66.347\n",
      "n_iter:172600 loss: 4.293 ppl: 73.198\n",
      "n_iter:172800 loss: 4.336 ppl: 76.395\n",
      "n_iter:173000 loss: 4.191 ppl: 66.060\n",
      "n_iter:173200 loss: 4.260 ppl: 70.796\n",
      "n_iter:173400 loss: 4.379 ppl: 79.738\n",
      "n_iter:173600 loss: 4.289 ppl: 72.902\n",
      "n_iter:173800 loss: 4.074 ppl: 58.794\n",
      "n_iter:174000 loss: 4.248 ppl: 69.948\n",
      "n_iter:174200 loss: 4.207 ppl: 67.172\n",
      "### find best mode ### 152.51126036401573\n",
      "validation vloss: 5.027 vppl: 152.511, best ppl: 152.511\n",
      "[24/50] epochs training...\n",
      "n_iter:174400 loss: 4.420 ppl: 83.095\n",
      "n_iter:174600 loss: 4.292 ppl: 73.140\n",
      "n_iter:174800 loss: 4.444 ppl: 85.148\n",
      "n_iter:175000 loss: 4.428 ppl: 83.780\n",
      "n_iter:175200 loss: 4.350 ppl: 77.457\n",
      "n_iter:175400 loss: 4.329 ppl: 75.879\n",
      "n_iter:175600 loss: 4.126 ppl: 61.938\n",
      "n_iter:175800 loss: 4.310 ppl: 74.420\n",
      "n_iter:176000 loss: 4.212 ppl: 67.472\n",
      "n_iter:176200 loss: 4.238 ppl: 69.285\n",
      "n_iter:176400 loss: 4.262 ppl: 70.920\n",
      "n_iter:176600 loss: 4.053 ppl: 57.577\n",
      "n_iter:176800 loss: 4.176 ppl: 65.102\n",
      "n_iter:177000 loss: 4.236 ppl: 69.137\n",
      "n_iter:177200 loss: 4.272 ppl: 71.669\n",
      "n_iter:177400 loss: 4.288 ppl: 72.825\n",
      "n_iter:177600 loss: 4.059 ppl: 57.945\n",
      "n_iter:177800 loss: 4.330 ppl: 75.976\n",
      "n_iter:178000 loss: 4.379 ppl: 79.758\n",
      "n_iter:178200 loss: 4.177 ppl: 65.198\n",
      "n_iter:178400 loss: 4.073 ppl: 58.706\n",
      "n_iter:178600 loss: 4.304 ppl: 74.020\n",
      "n_iter:178800 loss: 4.127 ppl: 61.982\n",
      "n_iter:179000 loss: 4.016 ppl: 55.484\n",
      "n_iter:179200 loss: 4.130 ppl: 62.148\n",
      "n_iter:179400 loss: 4.157 ppl: 63.891\n",
      "n_iter:179600 loss: 4.196 ppl: 66.453\n",
      "n_iter:179800 loss: 4.323 ppl: 75.402\n",
      "n_iter:180000 loss: 4.205 ppl: 67.011\n",
      "n_iter:180200 loss: 4.253 ppl: 70.302\n",
      "n_iter:180400 loss: 4.200 ppl: 66.670\n",
      "n_iter:180600 loss: 4.386 ppl: 80.355\n",
      "n_iter:180800 loss: 4.278 ppl: 72.089\n",
      "n_iter:181000 loss: 4.117 ppl: 61.353\n",
      "n_iter:181200 loss: 4.222 ppl: 68.176\n",
      "n_iter:181400 loss: 4.141 ppl: 62.857\n",
      "### find best mode ### 151.94578499186954\n",
      "validation vloss: 5.024 vppl: 151.946, best ppl: 151.946\n",
      "[25/50] epochs training...\n",
      "n_iter:181600 loss: 4.410 ppl: 82.286\n",
      "n_iter:181800 loss: 4.296 ppl: 73.439\n",
      "n_iter:182000 loss: 4.412 ppl: 82.468\n",
      "n_iter:182200 loss: 4.414 ppl: 82.626\n",
      "n_iter:182400 loss: 4.354 ppl: 77.790\n",
      "n_iter:182600 loss: 4.284 ppl: 72.523\n",
      "n_iter:182800 loss: 4.270 ppl: 71.539\n",
      "n_iter:183000 loss: 4.199 ppl: 66.630\n",
      "n_iter:183200 loss: 4.205 ppl: 67.044\n",
      "n_iter:183400 loss: 4.241 ppl: 69.478\n",
      "n_iter:183600 loss: 4.186 ppl: 65.789\n",
      "n_iter:183800 loss: 4.126 ppl: 61.910\n",
      "n_iter:184000 loss: 4.159 ppl: 63.986\n",
      "n_iter:184200 loss: 4.183 ppl: 65.565\n",
      "n_iter:184400 loss: 4.246 ppl: 69.804\n",
      "n_iter:184600 loss: 4.352 ppl: 77.617\n",
      "n_iter:184800 loss: 4.006 ppl: 54.944\n",
      "n_iter:185000 loss: 4.316 ppl: 74.900\n",
      "n_iter:185200 loss: 4.319 ppl: 75.083\n",
      "n_iter:185400 loss: 4.260 ppl: 70.819\n",
      "n_iter:185600 loss: 3.999 ppl: 54.544\n",
      "n_iter:185800 loss: 4.316 ppl: 74.853\n",
      "n_iter:186000 loss: 4.171 ppl: 64.754\n",
      "n_iter:186200 loss: 3.976 ppl: 53.321\n",
      "n_iter:186400 loss: 4.083 ppl: 59.315\n",
      "n_iter:186600 loss: 4.233 ppl: 68.910\n",
      "n_iter:186800 loss: 4.149 ppl: 63.385\n",
      "n_iter:187000 loss: 4.214 ppl: 67.637\n",
      "n_iter:187200 loss: 4.198 ppl: 66.549\n",
      "n_iter:187400 loss: 4.343 ppl: 76.908\n",
      "n_iter:187600 loss: 4.127 ppl: 61.981\n",
      "n_iter:187800 loss: 4.376 ppl: 79.548\n",
      "n_iter:188000 loss: 4.285 ppl: 72.598\n",
      "n_iter:188200 loss: 4.231 ppl: 68.764\n",
      "n_iter:188400 loss: 4.076 ppl: 58.895\n",
      "n_iter:188600 loss: 4.165 ppl: 64.389\n",
      "n_iter:188800 loss: 4.277 ppl: 72.003\n",
      "validation vloss: 5.027 vppl: 152.521, best ppl: 151.946\n",
      "[26/50] epochs training...\n",
      "n_iter:189000 loss: 4.368 ppl: 78.847\n",
      "n_iter:189200 loss: 4.316 ppl: 74.921\n",
      "n_iter:189400 loss: 4.431 ppl: 83.974\n",
      "n_iter:189600 loss: 4.333 ppl: 76.200\n",
      "n_iter:189800 loss: 4.307 ppl: 74.182\n",
      "n_iter:190000 loss: 4.357 ppl: 77.994\n",
      "n_iter:190200 loss: 4.079 ppl: 59.104\n",
      "n_iter:190400 loss: 4.229 ppl: 68.659\n",
      "n_iter:190600 loss: 4.237 ppl: 69.197\n",
      "n_iter:190800 loss: 4.188 ppl: 65.900\n",
      "n_iter:191000 loss: 4.125 ppl: 61.888\n",
      "n_iter:191200 loss: 4.143 ppl: 63.001\n",
      "n_iter:191400 loss: 4.157 ppl: 63.908\n",
      "n_iter:191600 loss: 4.250 ppl: 70.110\n",
      "n_iter:191800 loss: 4.235 ppl: 69.066\n",
      "n_iter:192000 loss: 4.079 ppl: 59.081\n",
      "n_iter:192200 loss: 4.214 ppl: 67.598\n",
      "n_iter:192400 loss: 4.338 ppl: 76.588\n",
      "n_iter:192600 loss: 4.336 ppl: 76.411\n",
      "n_iter:192800 loss: 4.045 ppl: 57.107\n",
      "n_iter:193000 loss: 4.183 ppl: 65.576\n",
      "n_iter:193200 loss: 4.188 ppl: 65.906\n",
      "n_iter:193400 loss: 4.035 ppl: 56.567\n",
      "n_iter:193600 loss: 4.025 ppl: 55.982\n",
      "n_iter:193800 loss: 4.196 ppl: 66.447\n",
      "n_iter:194000 loss: 4.125 ppl: 61.880\n",
      "n_iter:194200 loss: 4.190 ppl: 66.025\n",
      "n_iter:194400 loss: 4.218 ppl: 67.898\n",
      "n_iter:194600 loss: 4.329 ppl: 75.871\n",
      "n_iter:194800 loss: 4.150 ppl: 63.428\n",
      "n_iter:195000 loss: 4.242 ppl: 69.561\n",
      "n_iter:195200 loss: 4.330 ppl: 75.947\n",
      "n_iter:195400 loss: 4.256 ppl: 70.504\n",
      "n_iter:195600 loss: 4.017 ppl: 55.518\n",
      "n_iter:195800 loss: 4.249 ppl: 70.063\n",
      "n_iter:196000 loss: 4.167 ppl: 64.544\n",
      "validation vloss: 5.029 vppl: 152.723, best ppl: 151.946\n",
      "[27/50] epochs training...\n",
      "n_iter:196200 loss: 4.386 ppl: 80.287\n",
      "n_iter:196400 loss: 4.257 ppl: 70.566\n",
      "n_iter:196600 loss: 4.418 ppl: 82.953\n",
      "n_iter:196800 loss: 4.388 ppl: 80.483\n",
      "n_iter:197000 loss: 4.317 ppl: 74.958\n",
      "n_iter:197200 loss: 4.302 ppl: 73.851\n",
      "n_iter:197400 loss: 4.072 ppl: 58.663\n",
      "n_iter:197600 loss: 4.252 ppl: 70.216\n",
      "n_iter:197800 loss: 4.176 ppl: 65.115\n",
      "n_iter:198000 loss: 4.217 ppl: 67.858\n",
      "n_iter:198200 loss: 4.203 ppl: 66.882\n",
      "n_iter:198400 loss: 4.035 ppl: 56.567\n",
      "n_iter:198600 loss: 4.165 ppl: 64.424\n",
      "n_iter:198800 loss: 4.211 ppl: 67.391\n",
      "n_iter:199000 loss: 4.231 ppl: 68.813\n",
      "n_iter:199200 loss: 4.194 ppl: 66.266\n",
      "n_iter:199400 loss: 4.079 ppl: 59.060\n",
      "n_iter:199600 loss: 4.292 ppl: 73.131\n",
      "n_iter:199800 loss: 4.353 ppl: 77.716\n",
      "n_iter:200000 loss: 4.130 ppl: 62.202\n",
      "n_iter:200200 loss: 4.044 ppl: 57.077\n",
      "n_iter:200400 loss: 4.260 ppl: 70.820\n",
      "n_iter:200600 loss: 4.067 ppl: 58.402\n",
      "n_iter:200800 loss: 3.973 ppl: 53.154\n",
      "n_iter:201000 loss: 4.117 ppl: 61.400\n",
      "n_iter:201200 loss: 4.151 ppl: 63.513\n",
      "n_iter:201400 loss: 4.126 ppl: 61.930\n",
      "n_iter:201600 loss: 4.310 ppl: 74.419\n",
      "n_iter:201800 loss: 4.180 ppl: 65.358\n",
      "n_iter:202000 loss: 4.217 ppl: 67.846\n",
      "n_iter:202200 loss: 4.173 ppl: 64.904\n",
      "n_iter:202400 loss: 4.360 ppl: 78.234\n",
      "n_iter:202600 loss: 4.246 ppl: 69.840\n",
      "n_iter:202800 loss: 4.072 ppl: 58.674\n",
      "n_iter:203000 loss: 4.189 ppl: 65.932\n",
      "n_iter:203200 loss: 4.120 ppl: 61.586\n",
      "validation vloss: 5.029 vppl: 152.753, best ppl: 151.946\n",
      "[28/50] epochs training...\n",
      "n_iter:203400 loss: 4.353 ppl: 77.680\n",
      "n_iter:203600 loss: 4.252 ppl: 70.274\n",
      "n_iter:203800 loss: 4.401 ppl: 81.532\n",
      "n_iter:204000 loss: 4.405 ppl: 81.823\n",
      "n_iter:204200 loss: 4.311 ppl: 74.487\n",
      "n_iter:204400 loss: 4.263 ppl: 71.013\n",
      "n_iter:204600 loss: 4.197 ppl: 66.468\n",
      "n_iter:204800 loss: 4.192 ppl: 66.179\n",
      "n_iter:205000 loss: 4.187 ppl: 65.845\n",
      "n_iter:205200 loss: 4.200 ppl: 66.659\n",
      "n_iter:205400 loss: 4.157 ppl: 63.858\n",
      "n_iter:205600 loss: 4.090 ppl: 59.760\n",
      "n_iter:205800 loss: 4.126 ppl: 61.945\n",
      "n_iter:206000 loss: 4.144 ppl: 63.064\n",
      "n_iter:206200 loss: 4.215 ppl: 67.708\n",
      "n_iter:206400 loss: 4.306 ppl: 74.170\n",
      "n_iter:206600 loss: 3.983 ppl: 53.679\n",
      "n_iter:206800 loss: 4.260 ppl: 70.809\n",
      "n_iter:207000 loss: 4.320 ppl: 75.197\n",
      "n_iter:207200 loss: 4.207 ppl: 67.147\n",
      "n_iter:207400 loss: 3.977 ppl: 53.372\n",
      "n_iter:207600 loss: 4.275 ppl: 71.880\n",
      "n_iter:207800 loss: 4.138 ppl: 62.651\n",
      "n_iter:208000 loss: 3.956 ppl: 52.223\n",
      "n_iter:208200 loss: 4.051 ppl: 57.456\n",
      "n_iter:208400 loss: 4.217 ppl: 67.843\n",
      "n_iter:208600 loss: 4.109 ppl: 60.858\n",
      "n_iter:208800 loss: 4.195 ppl: 66.332\n",
      "n_iter:209000 loss: 4.182 ppl: 65.518\n",
      "n_iter:209200 loss: 4.301 ppl: 73.738\n",
      "n_iter:209400 loss: 4.125 ppl: 61.880\n",
      "n_iter:209600 loss: 4.353 ppl: 77.694\n",
      "n_iter:209800 loss: 4.234 ppl: 68.977\n",
      "n_iter:210000 loss: 4.158 ppl: 63.941\n",
      "n_iter:210200 loss: 4.089 ppl: 59.677\n",
      "n_iter:210400 loss: 4.110 ppl: 60.935\n",
      "n_iter:210600 loss: 4.264 ppl: 71.115\n",
      "validation vloss: 5.026 vppl: 152.375, best ppl: 151.946\n",
      "[29/50] epochs training...\n",
      "n_iter:210800 loss: 4.338 ppl: 76.517\n",
      "n_iter:211000 loss: 4.291 ppl: 73.065\n",
      "n_iter:211200 loss: 4.394 ppl: 80.975\n",
      "n_iter:211400 loss: 4.320 ppl: 75.189\n",
      "n_iter:211600 loss: 4.260 ppl: 70.812\n",
      "n_iter:211800 loss: 4.321 ppl: 75.286\n",
      "n_iter:212000 loss: 4.050 ppl: 57.387\n",
      "n_iter:212200 loss: 4.216 ppl: 67.785\n",
      "n_iter:212400 loss: 4.204 ppl: 66.964\n",
      "n_iter:212600 loss: 4.148 ppl: 63.331\n",
      "n_iter:212800 loss: 4.100 ppl: 60.327\n",
      "n_iter:213000 loss: 4.118 ppl: 61.420\n",
      "n_iter:213200 loss: 4.127 ppl: 62.010\n",
      "n_iter:213400 loss: 4.220 ppl: 68.067\n",
      "n_iter:213600 loss: 4.216 ppl: 67.730\n",
      "n_iter:213800 loss: 4.012 ppl: 55.274\n",
      "n_iter:214000 loss: 4.217 ppl: 67.796\n",
      "n_iter:214200 loss: 4.298 ppl: 73.526\n",
      "n_iter:214400 loss: 4.320 ppl: 75.201\n",
      "n_iter:214600 loss: 3.970 ppl: 52.968\n",
      "n_iter:214800 loss: 4.172 ppl: 64.864\n",
      "n_iter:215000 loss: 4.144 ppl: 63.025\n",
      "n_iter:215200 loss: 4.010 ppl: 55.132\n",
      "n_iter:215400 loss: 3.997 ppl: 54.433\n",
      "n_iter:215600 loss: 4.188 ppl: 65.859\n",
      "n_iter:215800 loss: 4.090 ppl: 59.741\n",
      "n_iter:216000 loss: 4.165 ppl: 64.417\n",
      "n_iter:216200 loss: 4.177 ppl: 65.178\n",
      "n_iter:216400 loss: 4.301 ppl: 73.797\n",
      "n_iter:216600 loss: 4.108 ppl: 60.817\n",
      "n_iter:216800 loss: 4.236 ppl: 69.109\n",
      "n_iter:217000 loss: 4.280 ppl: 72.254\n",
      "n_iter:217200 loss: 4.259 ppl: 70.726\n",
      "n_iter:217400 loss: 3.958 ppl: 52.364\n",
      "n_iter:217600 loss: 4.212 ppl: 67.522\n",
      "n_iter:217800 loss: 4.181 ppl: 65.402\n",
      "validation vloss: 5.029 vppl: 152.779, best ppl: 151.946\n",
      "[30/50] epochs training...\n",
      "n_iter:218000 loss: 4.332 ppl: 76.101\n",
      "n_iter:218200 loss: 4.258 ppl: 70.664\n",
      "n_iter:218400 loss: 4.380 ppl: 79.844\n",
      "n_iter:218600 loss: 4.332 ppl: 76.102\n",
      "n_iter:218800 loss: 4.299 ppl: 73.598\n",
      "n_iter:219000 loss: 4.286 ppl: 72.660\n",
      "n_iter:219200 loss: 4.045 ppl: 57.128\n",
      "n_iter:219400 loss: 4.198 ppl: 66.526\n",
      "n_iter:219600 loss: 4.166 ppl: 64.464\n",
      "n_iter:219800 loss: 4.171 ppl: 64.765\n",
      "n_iter:220000 loss: 4.156 ppl: 63.841\n",
      "n_iter:220200 loss: 4.051 ppl: 57.439\n",
      "n_iter:220400 loss: 4.100 ppl: 60.336\n",
      "n_iter:220600 loss: 4.198 ppl: 66.524\n",
      "n_iter:220800 loss: 4.211 ppl: 67.442\n",
      "n_iter:221000 loss: 4.111 ppl: 61.022\n",
      "n_iter:221200 loss: 4.077 ppl: 58.966\n",
      "n_iter:221400 loss: 4.291 ppl: 73.016\n",
      "n_iter:221600 loss: 4.321 ppl: 75.273\n",
      "n_iter:221800 loss: 4.088 ppl: 59.626\n",
      "n_iter:222000 loss: 4.017 ppl: 55.549\n",
      "n_iter:222200 loss: 4.210 ppl: 67.338\n",
      "n_iter:222400 loss: 4.038 ppl: 56.738\n",
      "n_iter:222600 loss: 3.952 ppl: 52.048\n",
      "n_iter:222800 loss: 4.099 ppl: 60.273\n",
      "n_iter:223000 loss: 4.137 ppl: 62.636\n",
      "n_iter:223200 loss: 4.110 ppl: 60.964\n",
      "n_iter:223400 loss: 4.266 ppl: 71.220\n",
      "n_iter:223600 loss: 4.162 ppl: 64.213\n",
      "n_iter:223800 loss: 4.173 ppl: 64.887\n",
      "n_iter:224000 loss: 4.155 ppl: 63.780\n",
      "n_iter:224200 loss: 4.328 ppl: 75.768\n",
      "n_iter:224400 loss: 4.228 ppl: 68.559\n",
      "n_iter:224600 loss: 4.051 ppl: 57.476\n",
      "n_iter:224800 loss: 4.167 ppl: 64.510\n",
      "n_iter:225000 loss: 4.104 ppl: 60.594\n",
      "validation vloss: 5.031 vppl: 153.078, best ppl: 151.946\n",
      "[31/50] epochs training...\n",
      "n_iter:225200 loss: 4.336 ppl: 76.408\n",
      "n_iter:225400 loss: 4.230 ppl: 68.693\n",
      "n_iter:225600 loss: 4.372 ppl: 79.164\n",
      "n_iter:225800 loss: 4.370 ppl: 79.037\n",
      "n_iter:226000 loss: 4.279 ppl: 72.167\n",
      "n_iter:226200 loss: 4.254 ppl: 70.364\n",
      "n_iter:226400 loss: 4.137 ppl: 62.608\n",
      "n_iter:226600 loss: 4.171 ppl: 64.767\n",
      "n_iter:226800 loss: 4.170 ppl: 64.699\n",
      "n_iter:227000 loss: 4.166 ppl: 64.455\n",
      "n_iter:227200 loss: 4.140 ppl: 62.818\n",
      "n_iter:227400 loss: 4.044 ppl: 57.071\n",
      "n_iter:227600 loss: 4.094 ppl: 60.001\n",
      "n_iter:227800 loss: 4.111 ppl: 60.977\n",
      "n_iter:228000 loss: 4.210 ppl: 67.378\n",
      "n_iter:228200 loss: 4.293 ppl: 73.161\n",
      "n_iter:228400 loss: 3.943 ppl: 51.562\n",
      "n_iter:228600 loss: 4.246 ppl: 69.854\n",
      "n_iter:228800 loss: 4.299 ppl: 73.647\n",
      "n_iter:229000 loss: 4.164 ppl: 64.333\n",
      "n_iter:229200 loss: 3.976 ppl: 53.307\n",
      "n_iter:229400 loss: 4.257 ppl: 70.596\n",
      "n_iter:229600 loss: 4.120 ppl: 61.540\n",
      "n_iter:229800 loss: 3.897 ppl: 49.250\n",
      "n_iter:230000 loss: 4.033 ppl: 56.402\n",
      "n_iter:230200 loss: 4.150 ppl: 63.458\n",
      "n_iter:230400 loss: 4.100 ppl: 60.342\n",
      "n_iter:230600 loss: 4.205 ppl: 67.051\n",
      "n_iter:230800 loss: 4.138 ppl: 62.671\n",
      "n_iter:231000 loss: 4.261 ppl: 70.863\n",
      "n_iter:231200 loss: 4.120 ppl: 61.555\n",
      "n_iter:231400 loss: 4.327 ppl: 75.731\n",
      "n_iter:231600 loss: 4.198 ppl: 66.580\n",
      "n_iter:231800 loss: 4.120 ppl: 61.541\n",
      "n_iter:232000 loss: 4.104 ppl: 60.579\n",
      "n_iter:232200 loss: 4.062 ppl: 58.103\n",
      "n_iter:232400 loss: 4.267 ppl: 71.310\n",
      "validation vloss: 5.027 vppl: 152.528, best ppl: 151.946\n",
      "[32/50] epochs training...\n",
      "n_iter:232600 loss: 4.302 ppl: 73.878\n",
      "n_iter:232800 loss: 4.282 ppl: 72.377\n",
      "n_iter:233000 loss: 4.357 ppl: 78.062\n",
      "n_iter:233200 loss: 4.297 ppl: 73.497\n",
      "n_iter:233400 loss: 4.239 ppl: 69.340\n",
      "n_iter:233600 loss: 4.271 ppl: 71.620\n",
      "n_iter:233800 loss: 4.041 ppl: 56.907\n",
      "n_iter:234000 loss: 4.178 ppl: 65.223\n",
      "n_iter:234200 loss: 4.163 ppl: 64.265\n",
      "n_iter:234400 loss: 4.110 ppl: 60.938\n",
      "n_iter:234600 loss: 4.090 ppl: 59.717\n",
      "n_iter:234800 loss: 4.101 ppl: 60.405\n",
      "n_iter:235000 loss: 4.125 ppl: 61.845\n",
      "n_iter:235200 loss: 4.187 ppl: 65.834\n",
      "n_iter:235400 loss: 4.225 ppl: 68.371\n",
      "n_iter:235600 loss: 3.963 ppl: 52.615\n",
      "n_iter:235800 loss: 4.197 ppl: 66.473\n",
      "n_iter:236000 loss: 4.277 ppl: 72.012\n",
      "n_iter:236200 loss: 4.282 ppl: 72.388\n",
      "n_iter:236400 loss: 3.948 ppl: 51.847\n",
      "n_iter:236600 loss: 4.181 ppl: 65.429\n",
      "n_iter:236800 loss: 4.111 ppl: 60.993\n",
      "n_iter:237000 loss: 3.997 ppl: 54.444\n",
      "n_iter:237200 loss: 3.985 ppl: 53.774\n",
      "n_iter:237400 loss: 4.162 ppl: 64.222\n",
      "n_iter:237600 loss: 4.051 ppl: 57.473\n",
      "n_iter:237800 loss: 4.156 ppl: 63.846\n",
      "n_iter:238000 loss: 4.138 ppl: 62.649\n",
      "n_iter:238200 loss: 4.283 ppl: 72.479\n",
      "n_iter:238400 loss: 4.069 ppl: 58.477\n",
      "n_iter:238600 loss: 4.223 ppl: 68.217\n",
      "n_iter:238800 loss: 4.244 ppl: 69.688\n",
      "n_iter:239000 loss: 4.219 ppl: 67.985\n",
      "n_iter:239200 loss: 3.953 ppl: 52.073\n",
      "n_iter:239400 loss: 4.164 ppl: 64.299\n",
      "n_iter:239600 loss: 4.172 ppl: 64.831\n",
      "validation vloss: 5.035 vppl: 153.689, best ppl: 151.946\n",
      "[33/50] epochs training...\n",
      "n_iter:239800 loss: 4.296 ppl: 73.428\n",
      "n_iter:240000 loss: 4.251 ppl: 70.173\n",
      "n_iter:240200 loss: 4.363 ppl: 78.464\n",
      "n_iter:240400 loss: 4.310 ppl: 74.473\n",
      "n_iter:240600 loss: 4.265 ppl: 71.136\n",
      "n_iter:240800 loss: 4.275 ppl: 71.874\n",
      "n_iter:241000 loss: 4.024 ppl: 55.943\n",
      "n_iter:241200 loss: 4.141 ppl: 62.855\n",
      "n_iter:241400 loss: 4.174 ppl: 65.000\n",
      "n_iter:241600 loss: 4.155 ppl: 63.751\n",
      "n_iter:241800 loss: 4.101 ppl: 60.424\n",
      "n_iter:242000 loss: 4.060 ppl: 57.968\n",
      "n_iter:242200 loss: 4.072 ppl: 58.668\n",
      "n_iter:242400 loss: 4.193 ppl: 66.248\n",
      "n_iter:242600 loss: 4.188 ppl: 65.896\n",
      "n_iter:242800 loss: 4.048 ppl: 57.288\n",
      "n_iter:243000 loss: 4.115 ppl: 61.223\n",
      "n_iter:243200 loss: 4.272 ppl: 71.695\n",
      "n_iter:243400 loss: 4.291 ppl: 73.060\n",
      "n_iter:243600 loss: 4.060 ppl: 57.991\n",
      "n_iter:243800 loss: 4.023 ppl: 55.871\n",
      "n_iter:244000 loss: 4.173 ppl: 64.896\n",
      "n_iter:244200 loss: 4.013 ppl: 55.339\n",
      "n_iter:244400 loss: 3.923 ppl: 50.559\n",
      "n_iter:244600 loss: 4.101 ppl: 60.394\n",
      "n_iter:244800 loss: 4.119 ppl: 61.512\n",
      "n_iter:245000 loss: 4.085 ppl: 59.472\n",
      "n_iter:245200 loss: 4.226 ppl: 68.440\n",
      "n_iter:245400 loss: 4.162 ppl: 64.231\n",
      "n_iter:245600 loss: 4.117 ppl: 61.367\n",
      "n_iter:245800 loss: 4.152 ppl: 63.556\n",
      "n_iter:246000 loss: 4.304 ppl: 73.972\n",
      "n_iter:246200 loss: 4.182 ppl: 65.475\n",
      "n_iter:246400 loss: 4.016 ppl: 55.472\n",
      "n_iter:246600 loss: 4.165 ppl: 64.381\n",
      "n_iter:246800 loss: 4.075 ppl: 58.865\n",
      "validation vloss: 5.035 vppl: 153.717, best ppl: 151.946\n",
      "[34/50] epochs training...\n",
      "n_iter:247000 loss: 4.323 ppl: 75.399\n",
      "n_iter:247200 loss: 4.194 ppl: 66.258\n",
      "n_iter:247400 loss: 4.333 ppl: 76.144\n",
      "n_iter:247600 loss: 4.364 ppl: 78.577\n",
      "n_iter:247800 loss: 4.282 ppl: 72.399\n",
      "n_iter:248000 loss: 4.221 ppl: 68.084\n",
      "n_iter:248200 loss: 4.093 ppl: 59.949\n",
      "n_iter:248400 loss: 4.175 ppl: 65.026\n",
      "n_iter:248600 loss: 4.143 ppl: 62.963\n",
      "n_iter:248800 loss: 4.146 ppl: 63.162\n",
      "n_iter:249000 loss: 4.111 ppl: 60.987\n",
      "n_iter:249200 loss: 4.024 ppl: 55.916\n",
      "n_iter:249400 loss: 4.076 ppl: 58.901\n",
      "n_iter:249600 loss: 4.098 ppl: 60.225\n",
      "n_iter:249800 loss: 4.190 ppl: 66.008\n",
      "n_iter:250000 loss: 4.259 ppl: 70.744\n",
      "n_iter:250200 loss: 3.935 ppl: 51.180\n",
      "n_iter:250400 loss: 4.227 ppl: 68.486\n",
      "n_iter:250600 loss: 4.280 ppl: 72.273\n",
      "n_iter:250800 loss: 4.136 ppl: 62.532\n",
      "n_iter:251000 loss: 3.938 ppl: 51.296\n",
      "n_iter:251200 loss: 4.222 ppl: 68.142\n",
      "n_iter:251400 loss: 4.094 ppl: 59.973\n",
      "n_iter:251600 loss: 3.895 ppl: 49.181\n",
      "n_iter:251800 loss: 4.021 ppl: 55.740\n",
      "n_iter:252000 loss: 4.119 ppl: 61.483\n",
      "n_iter:252200 loss: 4.091 ppl: 59.790\n",
      "n_iter:252400 loss: 4.212 ppl: 67.509\n",
      "n_iter:252600 loss: 4.100 ppl: 60.352\n",
      "n_iter:252800 loss: 4.214 ppl: 67.604\n",
      "n_iter:253000 loss: 4.106 ppl: 60.689\n",
      "n_iter:253200 loss: 4.302 ppl: 73.854\n",
      "n_iter:253400 loss: 4.188 ppl: 65.906\n",
      "n_iter:253600 loss: 4.086 ppl: 59.493\n",
      "n_iter:253800 loss: 4.083 ppl: 59.305\n",
      "n_iter:254000 loss: 4.045 ppl: 57.139\n",
      "n_iter:254200 loss: 4.243 ppl: 69.609\n",
      "validation vloss: 5.039 vppl: 154.281, best ppl: 151.946\n",
      "[35/50] epochs training...\n",
      "n_iter:254400 loss: 4.300 ppl: 73.684\n",
      "n_iter:254600 loss: 4.266 ppl: 71.219\n",
      "n_iter:254800 loss: 4.357 ppl: 78.058\n",
      "n_iter:255000 loss: 4.277 ppl: 72.018\n",
      "n_iter:255200 loss: 4.205 ppl: 67.005\n",
      "n_iter:255400 loss: 4.254 ppl: 70.383\n",
      "n_iter:255600 loss: 4.060 ppl: 57.968\n",
      "n_iter:255800 loss: 4.146 ppl: 63.169\n",
      "n_iter:256000 loss: 4.136 ppl: 62.577\n",
      "n_iter:256200 loss: 4.084 ppl: 59.363\n",
      "n_iter:256400 loss: 4.044 ppl: 57.060\n",
      "n_iter:256600 loss: 4.089 ppl: 59.702\n",
      "n_iter:256800 loss: 4.106 ppl: 60.677\n",
      "n_iter:257000 loss: 4.162 ppl: 64.189\n",
      "n_iter:257200 loss: 4.226 ppl: 68.419\n",
      "n_iter:257400 loss: 3.942 ppl: 51.533\n",
      "n_iter:257600 loss: 4.181 ppl: 65.420\n",
      "n_iter:257800 loss: 4.261 ppl: 70.899\n",
      "n_iter:258000 loss: 4.228 ppl: 68.551\n",
      "n_iter:258200 loss: 3.941 ppl: 51.480\n",
      "n_iter:258400 loss: 4.173 ppl: 64.904\n",
      "n_iter:258600 loss: 4.091 ppl: 59.828\n",
      "n_iter:258800 loss: 3.970 ppl: 53.004\n",
      "n_iter:259000 loss: 3.975 ppl: 53.256\n",
      "n_iter:259200 loss: 4.158 ppl: 63.974\n",
      "n_iter:259400 loss: 4.049 ppl: 57.334\n",
      "n_iter:259600 loss: 4.138 ppl: 62.660\n",
      "n_iter:259800 loss: 4.113 ppl: 61.154\n",
      "n_iter:260000 loss: 4.292 ppl: 73.090\n",
      "n_iter:260200 loss: 4.055 ppl: 57.658\n",
      "n_iter:260400 loss: 4.221 ppl: 68.109\n",
      "n_iter:260600 loss: 4.233 ppl: 68.897\n",
      "n_iter:260800 loss: 4.203 ppl: 66.857\n",
      "n_iter:261000 loss: 3.951 ppl: 51.979\n",
      "n_iter:261200 loss: 4.137 ppl: 62.612\n",
      "n_iter:261400 loss: 4.152 ppl: 63.531\n",
      "validation vloss: 5.034 vppl: 153.494, best ppl: 151.946\n",
      "[36/50] epochs training...\n",
      "n_iter:261600 loss: 4.267 ppl: 71.294\n",
      "n_iter:261800 loss: 4.239 ppl: 69.358\n",
      "n_iter:262000 loss: 4.351 ppl: 77.564\n",
      "n_iter:262200 loss: 4.265 ppl: 71.151\n",
      "n_iter:262400 loss: 4.243 ppl: 69.625\n",
      "n_iter:262600 loss: 4.267 ppl: 71.338\n",
      "n_iter:262800 loss: 3.983 ppl: 53.691\n",
      "n_iter:263000 loss: 4.127 ppl: 61.964\n",
      "n_iter:263200 loss: 4.179 ppl: 65.289\n",
      "n_iter:263400 loss: 4.110 ppl: 60.931\n",
      "n_iter:263600 loss: 4.066 ppl: 58.341\n",
      "n_iter:263800 loss: 4.059 ppl: 57.908\n",
      "n_iter:264000 loss: 4.061 ppl: 58.047\n",
      "n_iter:264200 loss: 4.156 ppl: 63.840\n",
      "n_iter:264400 loss: 4.157 ppl: 63.881\n",
      "n_iter:264600 loss: 4.035 ppl: 56.537\n",
      "n_iter:264800 loss: 4.107 ppl: 60.768\n",
      "n_iter:265000 loss: 4.240 ppl: 69.385\n",
      "n_iter:265200 loss: 4.274 ppl: 71.829\n",
      "n_iter:265400 loss: 4.022 ppl: 55.834\n",
      "n_iter:265600 loss: 4.040 ppl: 56.831\n",
      "n_iter:265800 loss: 4.140 ppl: 62.794\n",
      "n_iter:266000 loss: 3.988 ppl: 53.943\n",
      "n_iter:266200 loss: 3.943 ppl: 51.591\n",
      "n_iter:266400 loss: 4.094 ppl: 59.969\n",
      "n_iter:266600 loss: 4.109 ppl: 60.892\n",
      "n_iter:266800 loss: 4.071 ppl: 58.625\n",
      "n_iter:267000 loss: 4.203 ppl: 66.892\n",
      "n_iter:267200 loss: 4.174 ppl: 64.987\n",
      "n_iter:267400 loss: 4.087 ppl: 59.567\n",
      "n_iter:267600 loss: 4.135 ppl: 62.475\n",
      "n_iter:267800 loss: 4.266 ppl: 71.256\n",
      "n_iter:268000 loss: 4.163 ppl: 64.268\n",
      "n_iter:268200 loss: 4.007 ppl: 54.992\n",
      "n_iter:268400 loss: 4.145 ppl: 63.103\n",
      "n_iter:268600 loss: 4.063 ppl: 58.133\n",
      "validation vloss: 5.043 vppl: 154.884, best ppl: 151.946\n",
      "[37/50] epochs training...\n",
      "n_iter:268800 loss: 4.305 ppl: 74.033\n",
      "n_iter:269000 loss: 4.190 ppl: 66.028\n",
      "n_iter:269200 loss: 4.329 ppl: 75.843\n",
      "n_iter:269400 loss: 4.318 ppl: 75.074\n",
      "n_iter:269600 loss: 4.261 ppl: 70.900\n",
      "n_iter:269800 loss: 4.200 ppl: 66.696\n",
      "n_iter:270000 loss: 4.052 ppl: 57.501\n",
      "n_iter:270200 loss: 4.176 ppl: 65.097\n",
      "n_iter:270400 loss: 4.132 ppl: 62.322\n",
      "n_iter:270600 loss: 4.122 ppl: 61.712\n",
      "n_iter:270800 loss: 4.114 ppl: 61.220\n",
      "n_iter:271000 loss: 3.991 ppl: 54.092\n",
      "n_iter:271200 loss: 4.057 ppl: 57.819\n",
      "n_iter:271400 loss: 4.087 ppl: 59.548\n",
      "n_iter:271600 loss: 4.163 ppl: 64.260\n",
      "n_iter:271800 loss: 4.240 ppl: 69.403\n",
      "n_iter:272000 loss: 3.923 ppl: 50.560\n",
      "n_iter:272200 loss: 4.216 ppl: 67.763\n",
      "n_iter:272400 loss: 4.271 ppl: 71.618\n",
      "n_iter:272600 loss: 4.115 ppl: 61.270\n",
      "n_iter:272800 loss: 3.918 ppl: 50.314\n",
      "n_iter:273000 loss: 4.188 ppl: 65.903\n",
      "n_iter:273200 loss: 4.070 ppl: 58.557\n",
      "n_iter:273400 loss: 3.888 ppl: 48.834\n",
      "n_iter:273600 loss: 4.017 ppl: 55.558\n",
      "n_iter:273800 loss: 4.088 ppl: 59.602\n",
      "n_iter:274000 loss: 4.091 ppl: 59.787\n",
      "n_iter:274200 loss: 4.211 ppl: 67.400\n",
      "n_iter:274400 loss: 4.083 ppl: 59.353\n",
      "n_iter:274600 loss: 4.198 ppl: 66.570\n",
      "n_iter:274800 loss: 4.081 ppl: 59.204\n",
      "n_iter:275000 loss: 4.286 ppl: 72.654\n",
      "n_iter:275200 loss: 4.187 ppl: 65.820\n",
      "n_iter:275400 loss: 4.039 ppl: 56.762\n",
      "n_iter:275600 loss: 4.091 ppl: 59.816\n",
      "n_iter:275800 loss: 4.021 ppl: 55.777\n",
      "validation vloss: 5.039 vppl: 154.356, best ppl: 151.946\n",
      "[38/50] epochs training...\n",
      "n_iter:276000 loss: 4.254 ppl: 70.408\n",
      "n_iter:276200 loss: 4.258 ppl: 70.686\n",
      "n_iter:276400 loss: 4.286 ppl: 72.641\n",
      "n_iter:276600 loss: 4.315 ppl: 74.825\n",
      "n_iter:276800 loss: 4.235 ppl: 69.067\n",
      "n_iter:277000 loss: 4.192 ppl: 66.165\n",
      "n_iter:277200 loss: 4.218 ppl: 67.867\n",
      "n_iter:277400 loss: 4.054 ppl: 57.617\n",
      "n_iter:277600 loss: 4.113 ppl: 61.106\n",
      "n_iter:277800 loss: 4.128 ppl: 62.041\n",
      "n_iter:278000 loss: 4.089 ppl: 59.684\n",
      "n_iter:278200 loss: 4.014 ppl: 55.394\n",
      "n_iter:278400 loss: 4.078 ppl: 59.044\n",
      "n_iter:278600 loss: 4.082 ppl: 59.284\n",
      "n_iter:278800 loss: 4.143 ppl: 63.015\n",
      "n_iter:279000 loss: 4.218 ppl: 67.874\n",
      "n_iter:279200 loss: 3.929 ppl: 50.854\n",
      "n_iter:279400 loss: 4.193 ppl: 66.192\n",
      "n_iter:279600 loss: 4.231 ppl: 68.784\n",
      "n_iter:279800 loss: 4.202 ppl: 66.851\n",
      "n_iter:280000 loss: 3.921 ppl: 50.453\n",
      "n_iter:280200 loss: 4.183 ppl: 65.556\n",
      "n_iter:280400 loss: 4.068 ppl: 58.439\n",
      "n_iter:280600 loss: 3.926 ppl: 50.722\n",
      "n_iter:280800 loss: 3.950 ppl: 51.952\n",
      "n_iter:281000 loss: 4.132 ppl: 62.292\n",
      "n_iter:281200 loss: 4.039 ppl: 56.781\n",
      "n_iter:281400 loss: 4.114 ppl: 61.216\n",
      "n_iter:281600 loss: 4.105 ppl: 60.661\n",
      "n_iter:281800 loss: 4.273 ppl: 71.711\n",
      "n_iter:282000 loss: 4.027 ppl: 56.092\n",
      "n_iter:282200 loss: 4.230 ppl: 68.715\n",
      "n_iter:282400 loss: 4.206 ppl: 67.082\n",
      "n_iter:282600 loss: 4.146 ppl: 63.201\n",
      "n_iter:282800 loss: 3.963 ppl: 52.597\n",
      "n_iter:283000 loss: 4.082 ppl: 59.276\n",
      "n_iter:283200 loss: 4.152 ppl: 63.537\n",
      "validation vloss: 5.041 vppl: 154.659, best ppl: 151.946\n",
      "[39/50] epochs training...\n",
      "n_iter:283400 loss: 4.241 ppl: 69.450\n",
      "n_iter:283600 loss: 4.253 ppl: 70.311\n",
      "n_iter:283800 loss: 4.344 ppl: 77.048\n",
      "n_iter:284000 loss: 4.235 ppl: 69.045\n",
      "n_iter:284200 loss: 4.226 ppl: 68.412\n",
      "n_iter:284400 loss: 4.257 ppl: 70.593\n",
      "n_iter:284600 loss: 3.981 ppl: 53.579\n",
      "n_iter:284800 loss: 4.112 ppl: 61.087\n",
      "n_iter:285000 loss: 4.164 ppl: 64.349\n",
      "n_iter:285200 loss: 4.096 ppl: 60.083\n",
      "n_iter:285400 loss: 4.039 ppl: 56.752\n",
      "n_iter:285600 loss: 4.035 ppl: 56.565\n",
      "n_iter:285800 loss: 4.026 ppl: 56.054\n",
      "n_iter:286000 loss: 4.173 ppl: 64.920\n",
      "n_iter:286200 loss: 4.137 ppl: 62.616\n",
      "n_iter:286400 loss: 4.013 ppl: 55.327\n",
      "n_iter:286600 loss: 4.089 ppl: 59.659\n",
      "n_iter:286800 loss: 4.231 ppl: 68.817\n",
      "n_iter:287000 loss: 4.263 ppl: 71.025\n",
      "n_iter:287200 loss: 3.983 ppl: 53.695\n",
      "n_iter:287400 loss: 4.024 ppl: 55.917\n",
      "n_iter:287600 loss: 4.134 ppl: 62.442\n",
      "n_iter:287800 loss: 3.945 ppl: 51.693\n",
      "n_iter:288000 loss: 3.941 ppl: 51.467\n",
      "n_iter:288200 loss: 4.090 ppl: 59.753\n",
      "n_iter:288400 loss: 4.066 ppl: 58.325\n",
      "n_iter:288600 loss: 4.060 ppl: 57.948\n",
      "n_iter:288800 loss: 4.179 ppl: 65.280\n",
      "n_iter:289000 loss: 4.182 ppl: 65.468\n",
      "n_iter:289200 loss: 4.061 ppl: 58.031\n",
      "n_iter:289400 loss: 4.118 ppl: 61.467\n",
      "n_iter:289600 loss: 4.250 ppl: 70.087\n",
      "n_iter:289800 loss: 4.153 ppl: 63.650\n",
      "n_iter:290000 loss: 3.975 ppl: 53.272\n",
      "n_iter:290200 loss: 4.136 ppl: 62.556\n",
      "n_iter:290400 loss: 4.044 ppl: 57.045\n",
      "validation vloss: 5.040 vppl: 154.483, best ppl: 151.946\n",
      "[40/50] epochs training...\n",
      "n_iter:290600 loss: 4.286 ppl: 72.664\n",
      "n_iter:290800 loss: 4.191 ppl: 66.074\n",
      "n_iter:291000 loss: 4.323 ppl: 75.409\n",
      "n_iter:291200 loss: 4.295 ppl: 73.368\n",
      "n_iter:291400 loss: 4.227 ppl: 68.528\n",
      "n_iter:291600 loss: 4.221 ppl: 68.101\n",
      "n_iter:291800 loss: 4.020 ppl: 55.692\n",
      "n_iter:292000 loss: 4.166 ppl: 64.444\n",
      "n_iter:292200 loss: 4.103 ppl: 60.512\n",
      "n_iter:292400 loss: 4.119 ppl: 61.470\n",
      "n_iter:292600 loss: 4.098 ppl: 60.224\n",
      "n_iter:292800 loss: 3.959 ppl: 52.424\n",
      "n_iter:293000 loss: 4.036 ppl: 56.626\n",
      "n_iter:293200 loss: 4.108 ppl: 60.839\n",
      "n_iter:293400 loss: 4.142 ppl: 62.957\n",
      "n_iter:293600 loss: 4.174 ppl: 64.968\n",
      "n_iter:293800 loss: 3.938 ppl: 51.340\n",
      "n_iter:294000 loss: 4.188 ppl: 65.911\n",
      "n_iter:294200 loss: 4.260 ppl: 70.820\n",
      "n_iter:294400 loss: 4.070 ppl: 58.584\n",
      "n_iter:294600 loss: 3.948 ppl: 51.836\n",
      "n_iter:294800 loss: 4.169 ppl: 64.644\n",
      "n_iter:295000 loss: 4.039 ppl: 56.782\n",
      "n_iter:295200 loss: 3.895 ppl: 49.142\n",
      "n_iter:295400 loss: 4.005 ppl: 54.897\n",
      "n_iter:295600 loss: 4.050 ppl: 57.386\n",
      "n_iter:295800 loss: 4.085 ppl: 59.451\n",
      "n_iter:296000 loss: 4.213 ppl: 67.534\n",
      "n_iter:296200 loss: 4.067 ppl: 58.367\n",
      "n_iter:296400 loss: 4.148 ppl: 63.289\n",
      "n_iter:296600 loss: 4.105 ppl: 60.623\n",
      "n_iter:296800 loss: 4.248 ppl: 69.937\n",
      "n_iter:297000 loss: 4.176 ppl: 65.127\n",
      "n_iter:297200 loss: 4.023 ppl: 55.865\n",
      "n_iter:297400 loss: 4.074 ppl: 58.815\n",
      "n_iter:297600 loss: 4.002 ppl: 54.695\n",
      "validation vloss: 5.046 vppl: 155.352, best ppl: 151.946\n",
      "[41/50] epochs training...\n",
      "n_iter:297800 loss: 4.257 ppl: 70.586\n",
      "n_iter:298000 loss: 4.211 ppl: 67.402\n",
      "n_iter:298200 loss: 4.283 ppl: 72.473\n",
      "n_iter:298400 loss: 4.325 ppl: 75.540\n",
      "n_iter:298600 loss: 4.225 ppl: 68.388\n",
      "n_iter:298800 loss: 4.169 ppl: 64.680\n",
      "n_iter:299000 loss: 4.171 ppl: 64.790\n",
      "n_iter:299200 loss: 4.059 ppl: 57.904\n",
      "n_iter:299400 loss: 4.095 ppl: 60.016\n",
      "n_iter:299600 loss: 4.116 ppl: 61.340\n",
      "n_iter:299800 loss: 4.086 ppl: 59.484\n",
      "n_iter:300000 loss: 4.003 ppl: 54.747\n",
      "n_iter:300200 loss: 4.054 ppl: 57.620\n",
      "n_iter:300400 loss: 4.052 ppl: 57.506\n",
      "n_iter:300600 loss: 4.116 ppl: 61.326\n",
      "n_iter:300800 loss: 4.212 ppl: 67.470\n",
      "n_iter:301000 loss: 3.909 ppl: 49.836\n",
      "n_iter:301200 loss: 4.186 ppl: 65.744\n",
      "n_iter:301400 loss: 4.213 ppl: 67.536\n",
      "n_iter:301600 loss: 4.164 ppl: 64.339\n",
      "n_iter:301800 loss: 3.899 ppl: 49.353\n",
      "n_iter:302000 loss: 4.172 ppl: 64.860\n",
      "n_iter:302200 loss: 4.075 ppl: 58.832\n",
      "n_iter:302400 loss: 3.897 ppl: 49.234\n",
      "n_iter:302600 loss: 3.963 ppl: 52.630\n",
      "n_iter:302800 loss: 4.112 ppl: 61.068\n",
      "n_iter:303000 loss: 4.041 ppl: 56.877\n",
      "n_iter:303200 loss: 4.107 ppl: 60.759\n",
      "n_iter:303400 loss: 4.078 ppl: 58.999\n",
      "n_iter:303600 loss: 4.244 ppl: 69.718\n",
      "n_iter:303800 loss: 4.004 ppl: 54.808\n",
      "n_iter:304000 loss: 4.233 ppl: 68.948\n",
      "n_iter:304200 loss: 4.160 ppl: 64.066\n",
      "n_iter:304400 loss: 4.135 ppl: 62.459\n",
      "n_iter:304600 loss: 3.968 ppl: 52.863\n",
      "n_iter:304800 loss: 4.055 ppl: 57.662\n",
      "n_iter:305000 loss: 4.159 ppl: 64.004\n",
      "validation vloss: 5.040 vppl: 154.495, best ppl: 151.946\n",
      "[42/50] epochs training...\n",
      "n_iter:305200 loss: 4.243 ppl: 69.593\n",
      "n_iter:305400 loss: 4.232 ppl: 68.863\n",
      "n_iter:305600 loss: 4.342 ppl: 76.899\n",
      "n_iter:305800 loss: 4.199 ppl: 66.618\n",
      "n_iter:306000 loss: 4.214 ppl: 67.630\n",
      "n_iter:306200 loss: 4.243 ppl: 69.648\n",
      "n_iter:306400 loss: 3.986 ppl: 53.841\n",
      "n_iter:306600 loss: 4.112 ppl: 61.069\n",
      "n_iter:306800 loss: 4.169 ppl: 64.653\n",
      "n_iter:307000 loss: 4.091 ppl: 59.799\n",
      "n_iter:307200 loss: 4.006 ppl: 54.918\n",
      "n_iter:307400 loss: 4.025 ppl: 55.994\n",
      "n_iter:307600 loss: 4.038 ppl: 56.733\n",
      "n_iter:307800 loss: 4.130 ppl: 62.174\n",
      "n_iter:308000 loss: 4.132 ppl: 62.286\n",
      "n_iter:308200 loss: 3.979 ppl: 53.449\n",
      "n_iter:308400 loss: 4.099 ppl: 60.285\n",
      "n_iter:308600 loss: 4.218 ppl: 67.884\n",
      "n_iter:308800 loss: 4.239 ppl: 69.321\n",
      "n_iter:309000 loss: 3.971 ppl: 53.052\n",
      "n_iter:309200 loss: 4.033 ppl: 56.421\n",
      "n_iter:309400 loss: 4.112 ppl: 61.065\n",
      "n_iter:309600 loss: 3.941 ppl: 51.484\n",
      "n_iter:309800 loss: 3.935 ppl: 51.157\n",
      "n_iter:310000 loss: 4.086 ppl: 59.474\n",
      "n_iter:310200 loss: 4.022 ppl: 55.792\n",
      "n_iter:310400 loss: 4.067 ppl: 58.385\n",
      "n_iter:310600 loss: 4.139 ppl: 62.727\n",
      "n_iter:310800 loss: 4.199 ppl: 66.612\n",
      "n_iter:311000 loss: 4.038 ppl: 56.696\n",
      "n_iter:311200 loss: 4.125 ppl: 61.890\n",
      "n_iter:311400 loss: 4.220 ppl: 68.057\n",
      "n_iter:311600 loss: 4.148 ppl: 63.277\n",
      "n_iter:311800 loss: 3.944 ppl: 51.620\n",
      "n_iter:312000 loss: 4.112 ppl: 61.075\n",
      "n_iter:312200 loss: 4.054 ppl: 57.624\n",
      "validation vloss: 5.043 vppl: 154.997, best ppl: 151.946\n",
      "[43/50] epochs training...\n",
      "n_iter:312400 loss: 4.270 ppl: 71.532\n",
      "n_iter:312600 loss: 4.155 ppl: 63.769\n",
      "n_iter:312800 loss: 4.321 ppl: 75.263\n",
      "n_iter:313000 loss: 4.296 ppl: 73.401\n",
      "n_iter:313200 loss: 4.220 ppl: 68.017\n",
      "n_iter:313400 loss: 4.196 ppl: 66.396\n",
      "n_iter:313600 loss: 3.991 ppl: 54.095\n",
      "n_iter:313800 loss: 4.190 ppl: 66.050\n",
      "n_iter:314000 loss: 4.083 ppl: 59.305\n",
      "n_iter:314200 loss: 4.100 ppl: 60.368\n",
      "n_iter:314400 loss: 4.118 ppl: 61.462\n",
      "n_iter:314600 loss: 3.931 ppl: 50.945\n",
      "n_iter:314800 loss: 4.042 ppl: 56.952\n",
      "n_iter:315000 loss: 4.086 ppl: 59.486\n",
      "n_iter:315200 loss: 4.127 ppl: 61.973\n",
      "n_iter:315400 loss: 4.120 ppl: 61.582\n",
      "n_iter:315600 loss: 3.953 ppl: 52.110\n",
      "n_iter:315800 loss: 4.189 ppl: 65.960\n",
      "n_iter:316000 loss: 4.247 ppl: 69.907\n",
      "n_iter:316200 loss: 4.046 ppl: 57.183\n",
      "n_iter:316400 loss: 3.950 ppl: 51.912\n",
      "n_iter:316600 loss: 4.146 ppl: 63.203\n",
      "n_iter:316800 loss: 3.993 ppl: 54.205\n",
      "n_iter:317000 loss: 3.897 ppl: 49.275\n",
      "n_iter:317200 loss: 3.992 ppl: 54.138\n",
      "n_iter:317400 loss: 4.036 ppl: 56.572\n",
      "n_iter:317600 loss: 4.064 ppl: 58.190\n",
      "n_iter:317800 loss: 4.206 ppl: 67.077\n",
      "n_iter:318000 loss: 4.061 ppl: 58.004\n",
      "n_iter:318200 loss: 4.137 ppl: 62.611\n",
      "n_iter:318400 loss: 4.082 ppl: 59.260\n",
      "n_iter:318600 loss: 4.251 ppl: 70.158\n",
      "n_iter:318800 loss: 4.144 ppl: 63.030\n",
      "n_iter:319000 loss: 3.998 ppl: 54.486\n",
      "n_iter:319200 loss: 4.078 ppl: 59.034\n",
      "n_iter:319400 loss: 3.989 ppl: 53.978\n",
      "validation vloss: 5.049 vppl: 155.860, best ppl: 151.946\n",
      "[44/50] epochs training...\n",
      "n_iter:319600 loss: 4.235 ppl: 69.065\n",
      "n_iter:319800 loss: 4.158 ppl: 63.950\n",
      "n_iter:320000 loss: 4.297 ppl: 73.446\n",
      "n_iter:320200 loss: 4.296 ppl: 73.413\n",
      "n_iter:320400 loss: 4.211 ppl: 67.428\n",
      "n_iter:320600 loss: 4.172 ppl: 64.852\n",
      "n_iter:320800 loss: 4.129 ppl: 62.089\n",
      "n_iter:321000 loss: 4.074 ppl: 58.817\n",
      "n_iter:321200 loss: 4.072 ppl: 58.699\n",
      "n_iter:321400 loss: 4.112 ppl: 61.068\n",
      "n_iter:321600 loss: 4.082 ppl: 59.271\n",
      "n_iter:321800 loss: 3.979 ppl: 53.469\n",
      "n_iter:322000 loss: 4.045 ppl: 57.125\n",
      "n_iter:322200 loss: 4.037 ppl: 56.672\n",
      "n_iter:322400 loss: 4.092 ppl: 59.870\n",
      "n_iter:322600 loss: 4.210 ppl: 67.381\n",
      "n_iter:322800 loss: 3.895 ppl: 49.142\n",
      "n_iter:323000 loss: 4.172 ppl: 64.844\n",
      "n_iter:323200 loss: 4.194 ppl: 66.287\n",
      "n_iter:323400 loss: 4.154 ppl: 63.662\n",
      "n_iter:323600 loss: 3.891 ppl: 48.939\n",
      "n_iter:323800 loss: 4.169 ppl: 64.652\n",
      "n_iter:324000 loss: 4.054 ppl: 57.640\n",
      "n_iter:324200 loss: 3.867 ppl: 47.818\n",
      "n_iter:324400 loss: 3.956 ppl: 52.243\n",
      "n_iter:324600 loss: 4.116 ppl: 61.294\n",
      "n_iter:324800 loss: 4.015 ppl: 55.425\n",
      "n_iter:325000 loss: 4.110 ppl: 60.926\n",
      "n_iter:325200 loss: 4.077 ppl: 58.987\n",
      "n_iter:325400 loss: 4.214 ppl: 67.614\n",
      "n_iter:325600 loss: 4.004 ppl: 54.797\n",
      "n_iter:325800 loss: 4.230 ppl: 68.683\n",
      "n_iter:326000 loss: 4.135 ppl: 62.507\n",
      "n_iter:326200 loss: 4.086 ppl: 59.525\n",
      "n_iter:326400 loss: 3.987 ppl: 53.889\n",
      "n_iter:326600 loss: 4.035 ppl: 56.537\n",
      "n_iter:326800 loss: 4.146 ppl: 63.188\n",
      "validation vloss: 5.047 vppl: 155.587, best ppl: 151.946\n",
      "[45/50] epochs training...\n",
      "n_iter:327000 loss: 4.240 ppl: 69.401\n",
      "n_iter:327200 loss: 4.213 ppl: 67.526\n",
      "n_iter:327400 loss: 4.311 ppl: 74.541\n",
      "n_iter:327600 loss: 4.215 ppl: 67.702\n",
      "n_iter:327800 loss: 4.173 ppl: 64.912\n",
      "n_iter:328000 loss: 4.219 ppl: 67.958\n",
      "n_iter:328200 loss: 3.944 ppl: 51.650\n",
      "n_iter:328400 loss: 4.117 ppl: 61.372\n",
      "n_iter:328600 loss: 4.142 ppl: 62.914\n",
      "n_iter:328800 loss: 4.062 ppl: 58.109\n",
      "n_iter:329000 loss: 4.009 ppl: 55.098\n",
      "n_iter:329200 loss: 4.026 ppl: 56.063\n",
      "n_iter:329400 loss: 4.031 ppl: 56.306\n",
      "n_iter:329600 loss: 4.119 ppl: 61.483\n",
      "n_iter:329800 loss: 4.092 ppl: 59.840\n",
      "n_iter:330000 loss: 3.955 ppl: 52.181\n",
      "n_iter:330200 loss: 4.116 ppl: 61.327\n",
      "n_iter:330400 loss: 4.213 ppl: 67.563\n",
      "n_iter:330600 loss: 4.208 ppl: 67.196\n",
      "n_iter:330800 loss: 3.925 ppl: 50.675\n",
      "n_iter:331000 loss: 4.047 ppl: 57.251\n",
      "n_iter:331200 loss: 4.082 ppl: 59.257\n",
      "n_iter:331400 loss: 3.935 ppl: 51.186\n",
      "n_iter:331600 loss: 3.925 ppl: 50.639\n",
      "n_iter:331800 loss: 4.090 ppl: 59.755\n",
      "n_iter:332000 loss: 4.017 ppl: 55.518\n",
      "n_iter:332200 loss: 4.076 ppl: 58.930\n",
      "n_iter:332400 loss: 4.108 ppl: 60.798\n",
      "n_iter:332600 loss: 4.202 ppl: 66.828\n",
      "n_iter:332800 loss: 4.024 ppl: 55.942\n",
      "n_iter:333000 loss: 4.126 ppl: 61.920\n",
      "n_iter:333200 loss: 4.205 ppl: 67.019\n",
      "n_iter:333400 loss: 4.148 ppl: 63.289\n",
      "n_iter:333600 loss: 3.908 ppl: 49.824\n",
      "n_iter:333800 loss: 4.122 ppl: 61.690\n",
      "n_iter:334000 loss: 4.053 ppl: 57.581\n",
      "validation vloss: 5.049 vppl: 155.932, best ppl: 151.946\n",
      "[46/50] epochs training...\n",
      "n_iter:334200 loss: 4.251 ppl: 70.157\n",
      "n_iter:334400 loss: 4.153 ppl: 63.634\n",
      "n_iter:334600 loss: 4.323 ppl: 75.397\n",
      "n_iter:334800 loss: 4.251 ppl: 70.197\n",
      "n_iter:335000 loss: 4.213 ppl: 67.529\n",
      "n_iter:335200 loss: 4.203 ppl: 66.899\n",
      "n_iter:335400 loss: 3.972 ppl: 53.082\n",
      "n_iter:335600 loss: 4.121 ppl: 61.626\n",
      "n_iter:335800 loss: 4.087 ppl: 59.546\n",
      "n_iter:336000 loss: 4.104 ppl: 60.612\n",
      "n_iter:336200 loss: 4.097 ppl: 60.190\n",
      "n_iter:336400 loss: 3.929 ppl: 50.855\n",
      "n_iter:336600 loss: 4.042 ppl: 56.957\n",
      "n_iter:336800 loss: 4.079 ppl: 59.057\n",
      "n_iter:337000 loss: 4.123 ppl: 61.732\n",
      "n_iter:337200 loss: 4.053 ppl: 57.592\n",
      "n_iter:337400 loss: 4.003 ppl: 54.768\n",
      "n_iter:337600 loss: 4.190 ppl: 66.011\n",
      "n_iter:337800 loss: 4.227 ppl: 68.542\n",
      "n_iter:338000 loss: 4.027 ppl: 56.106\n",
      "n_iter:338200 loss: 3.948 ppl: 51.808\n",
      "n_iter:338400 loss: 4.130 ppl: 62.189\n",
      "n_iter:338600 loss: 3.977 ppl: 53.349\n",
      "n_iter:338800 loss: 3.865 ppl: 47.719\n",
      "n_iter:339000 loss: 4.011 ppl: 55.177\n",
      "n_iter:339200 loss: 4.036 ppl: 56.616\n",
      "n_iter:339400 loss: 4.031 ppl: 56.335\n",
      "n_iter:339600 loss: 4.209 ppl: 67.295\n",
      "n_iter:339800 loss: 4.059 ppl: 57.888\n",
      "n_iter:340000 loss: 4.095 ppl: 60.027\n",
      "n_iter:340200 loss: 4.072 ppl: 58.664\n",
      "n_iter:340400 loss: 4.253 ppl: 70.331\n",
      "n_iter:340600 loss: 4.149 ppl: 63.387\n",
      "n_iter:340800 loss: 3.976 ppl: 53.281\n",
      "n_iter:341000 loss: 4.064 ppl: 58.235\n",
      "n_iter:341200 loss: 4.010 ppl: 55.153\n",
      "validation vloss: 5.048 vppl: 155.740, best ppl: 151.946\n",
      "[47/50] epochs training...\n",
      "n_iter:341400 loss: 4.230 ppl: 68.689\n",
      "n_iter:341600 loss: 4.146 ppl: 63.195\n",
      "n_iter:341800 loss: 4.291 ppl: 73.042\n",
      "n_iter:342000 loss: 4.292 ppl: 73.099\n",
      "n_iter:342200 loss: 4.194 ppl: 66.261\n",
      "n_iter:342400 loss: 4.171 ppl: 64.772\n",
      "n_iter:342600 loss: 4.089 ppl: 59.696\n",
      "n_iter:342800 loss: 4.086 ppl: 59.480\n",
      "n_iter:343000 loss: 4.083 ppl: 59.298\n",
      "n_iter:343200 loss: 4.102 ppl: 60.485\n",
      "n_iter:343400 loss: 4.065 ppl: 58.239\n",
      "n_iter:343600 loss: 3.978 ppl: 53.394\n",
      "n_iter:343800 loss: 4.016 ppl: 55.478\n",
      "n_iter:344000 loss: 4.050 ppl: 57.370\n",
      "n_iter:344200 loss: 4.104 ppl: 60.557\n",
      "n_iter:344400 loss: 4.192 ppl: 66.170\n",
      "n_iter:344600 loss: 3.889 ppl: 48.882\n",
      "n_iter:344800 loss: 4.162 ppl: 64.193\n",
      "n_iter:345000 loss: 4.208 ppl: 67.192\n",
      "n_iter:345200 loss: 4.097 ppl: 60.156\n",
      "n_iter:345400 loss: 3.900 ppl: 49.379\n",
      "n_iter:345600 loss: 4.159 ppl: 63.981\n",
      "n_iter:345800 loss: 4.048 ppl: 57.284\n",
      "n_iter:346000 loss: 3.862 ppl: 47.584\n",
      "n_iter:346200 loss: 3.945 ppl: 51.691\n",
      "n_iter:346400 loss: 4.103 ppl: 60.512\n",
      "n_iter:346600 loss: 4.027 ppl: 56.088\n",
      "n_iter:346800 loss: 4.093 ppl: 59.933\n",
      "n_iter:347000 loss: 4.067 ppl: 58.401\n",
      "n_iter:347200 loss: 4.175 ppl: 65.021\n",
      "n_iter:347400 loss: 4.022 ppl: 55.833\n",
      "n_iter:347600 loss: 4.242 ppl: 69.530\n",
      "n_iter:347800 loss: 4.116 ppl: 61.306\n",
      "n_iter:348000 loss: 4.056 ppl: 57.752\n",
      "n_iter:348200 loss: 4.018 ppl: 55.611\n",
      "n_iter:348400 loss: 3.995 ppl: 54.331\n",
      "n_iter:348600 loss: 4.142 ppl: 62.941\n",
      "validation vloss: 5.048 vppl: 155.736, best ppl: 151.946\n",
      "[48/50] epochs training...\n",
      "n_iter:348800 loss: 4.225 ppl: 68.342\n",
      "n_iter:349000 loss: 4.191 ppl: 66.097\n",
      "n_iter:349200 loss: 4.310 ppl: 74.435\n",
      "n_iter:349400 loss: 4.204 ppl: 66.938\n",
      "n_iter:349600 loss: 4.175 ppl: 65.017\n",
      "n_iter:349800 loss: 4.203 ppl: 66.854\n",
      "n_iter:350000 loss: 3.957 ppl: 52.319\n",
      "n_iter:350200 loss: 4.112 ppl: 61.049\n",
      "n_iter:350400 loss: 4.100 ppl: 60.329\n",
      "n_iter:350600 loss: 4.058 ppl: 57.840\n",
      "n_iter:350800 loss: 4.009 ppl: 55.101\n",
      "n_iter:351000 loss: 4.016 ppl: 55.461\n",
      "n_iter:351200 loss: 4.043 ppl: 56.971\n",
      "n_iter:351400 loss: 4.103 ppl: 60.496\n",
      "n_iter:351600 loss: 4.122 ppl: 61.695\n",
      "n_iter:351800 loss: 3.921 ppl: 50.474\n",
      "n_iter:352000 loss: 4.136 ppl: 62.562\n",
      "n_iter:352200 loss: 4.191 ppl: 66.113\n",
      "n_iter:352400 loss: 4.204 ppl: 66.984\n",
      "n_iter:352600 loss: 3.876 ppl: 48.240\n",
      "n_iter:352800 loss: 4.076 ppl: 58.903\n",
      "n_iter:353000 loss: 4.024 ppl: 55.926\n",
      "n_iter:353200 loss: 3.932 ppl: 50.985\n",
      "n_iter:353400 loss: 3.912 ppl: 50.001\n",
      "n_iter:353600 loss: 4.091 ppl: 59.822\n",
      "n_iter:353800 loss: 3.995 ppl: 54.320\n",
      "n_iter:354000 loss: 4.082 ppl: 59.268\n",
      "n_iter:354200 loss: 4.071 ppl: 58.627\n",
      "n_iter:354400 loss: 4.203 ppl: 66.863\n",
      "n_iter:354600 loss: 4.008 ppl: 55.012\n",
      "n_iter:354800 loss: 4.137 ppl: 62.614\n",
      "n_iter:355000 loss: 4.169 ppl: 64.623\n",
      "n_iter:355200 loss: 4.160 ppl: 64.099\n",
      "n_iter:355400 loss: 3.882 ppl: 48.500\n",
      "n_iter:355600 loss: 4.107 ppl: 60.774\n",
      "n_iter:355800 loss: 4.074 ppl: 58.803\n",
      "validation vloss: 5.049 vppl: 155.938, best ppl: 151.946\n",
      "[49/50] epochs training...\n",
      "n_iter:356000 loss: 4.215 ppl: 67.703\n",
      "n_iter:356200 loss: 4.164 ppl: 64.319\n",
      "n_iter:356400 loss: 4.293 ppl: 73.200\n",
      "n_iter:356600 loss: 4.222 ppl: 68.139\n",
      "n_iter:356800 loss: 4.207 ppl: 67.179\n",
      "n_iter:357000 loss: 4.194 ppl: 66.276\n",
      "n_iter:357200 loss: 3.952 ppl: 52.044\n",
      "n_iter:357400 loss: 4.098 ppl: 60.205\n",
      "n_iter:357600 loss: 4.085 ppl: 59.460\n",
      "n_iter:357800 loss: 4.085 ppl: 59.425\n",
      "n_iter:358000 loss: 4.061 ppl: 58.024\n",
      "n_iter:358200 loss: 3.944 ppl: 51.629\n",
      "n_iter:358400 loss: 4.013 ppl: 55.302\n",
      "n_iter:358600 loss: 4.108 ppl: 60.803\n",
      "n_iter:358800 loss: 4.111 ppl: 60.992\n",
      "n_iter:359000 loss: 4.000 ppl: 54.602\n",
      "n_iter:359200 loss: 4.020 ppl: 55.682\n",
      "n_iter:359400 loss: 4.198 ppl: 66.557\n",
      "n_iter:359600 loss: 4.215 ppl: 67.669\n",
      "n_iter:359800 loss: 3.995 ppl: 54.323\n",
      "n_iter:360000 loss: 3.947 ppl: 51.776\n",
      "n_iter:360200 loss: 4.105 ppl: 60.620\n",
      "n_iter:360400 loss: 3.959 ppl: 52.392\n",
      "n_iter:360600 loss: 3.872 ppl: 48.055\n",
      "n_iter:360800 loss: 4.006 ppl: 54.952\n",
      "n_iter:361000 loss: 4.040 ppl: 56.799\n",
      "n_iter:361200 loss: 4.036 ppl: 56.618\n",
      "n_iter:361400 loss: 4.170 ppl: 64.699\n",
      "n_iter:361600 loss: 4.061 ppl: 58.056\n",
      "n_iter:361800 loss: 4.076 ppl: 58.891\n",
      "n_iter:362000 loss: 4.075 ppl: 58.847\n",
      "n_iter:362200 loss: 4.224 ppl: 68.331\n",
      "n_iter:362400 loss: 4.137 ppl: 62.591\n",
      "n_iter:362600 loss: 3.971 ppl: 53.034\n",
      "n_iter:362800 loss: 4.060 ppl: 58.002\n",
      "n_iter:363000 loss: 4.008 ppl: 55.060\n",
      "validation vloss: 5.053 vppl: 156.500, best ppl: 151.946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_iter, train_loss, valid_loss, best_ppl = 0, 0., 0., float('inf')\n",
    "for ep in range(epochs):\n",
    "    print(f\"[{ep}/{epochs}] epochs training...\")\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    for batch in trainloader:\n",
    "        n_iter += 1\n",
    "        batch = batch.transpose(1, 0).contiguous().to(device)\n",
    "        \n",
    "        target = batch[:, 1:].clone()\n",
    "        logits = model(batch[:, :-1])\n",
    "        loss = F.cross_entropy(logits.reshape(-1, vocab.size), target.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if n_iter % interval_print == 0:\n",
    "            train_loss /= interval_print\n",
    "            train_ppl = math.exp(train_loss)\n",
    "            print(f\"n_iter:{n_iter} loss: {train_loss:0.3f} ppl: {train_ppl:0.3f}\")\n",
    "            train_loss = 0\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(validloader, 1):\n",
    "        batch = batch.transpose(1, 0).to(device)\n",
    "        with torch.no_grad():\n",
    "            target = batch[:, 1:].clone()\n",
    "            logits = model(batch[:, :-1])\n",
    "            loss = F.cross_entropy(logits.reshape(-1, vocab.size), target.reshape(-1))\n",
    "            valid_loss += loss.item()\n",
    "    valid_loss = valid_loss/step\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "\n",
    "    if valid_ppl < best_ppl:\n",
    "        best_ppl = valid_ppl\n",
    "        torch.save(model, \"rnnlm-best.pth\")\n",
    "        print(\"### find best mode ###\", best_ppl)\n",
    "\n",
    "    print(f\"validation vloss: {valid_loss:0.3f} vppl: {valid_ppl:0.3f}, best ppl: {best_ppl:0.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup: i just think\n",
      "generated: i just think that i 'm not going to be able to see anything <eos> i 'm not going to be able to\n"
     ]
    }
   ],
   "source": [
    "max_len = 20\n",
    "generated = warmup = \"i just think\".lower()\n",
    "best_model = torch.load('./rnnlm-best.pth')\n",
    "best_model.eval()\n",
    "\n",
    "for _ in range(max_len):\n",
    "    inputs = torch.tensor([vocab.encode_line(generated, add_eos=False)]).to(device)\n",
    "    logits = F.softmax(best_model(inputs), dim=-1)[:, -1, :]\n",
    "    predicted_tok = vocab.id2tok[logits.argmax(-1).item()]\n",
    "    generated = \" \".join([generated, predicted_tok])\n",
    "\n",
    "print(f\"warmup: {warmup}\")\n",
    "print(f\"generated: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e235b1f40ab561dc697a518ea04835f082f50dd8c1dee948e78ceb42cbdb5e37"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
