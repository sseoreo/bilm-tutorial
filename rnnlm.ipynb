{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataset import LMDataset\n",
    "from vocab import Vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"ptb\"\n",
    "epochs = 10\n",
    "batch_length = 16\n",
    "batch_size = 8\n",
    "lr = .001\n",
    "\n",
    "n_layers = 2\n",
    "d_emb = 200\n",
    "d_hid = 250\n",
    "p_drop = 0.2\n",
    "\n",
    "interval_print = 200\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42068/42068 [00:00<00:00, 121466.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 50770), ('<unk>', 45020), ('N', 32481), ('of', 24400), ('to', 23638), ('a', 21196), ('in', 18000), ('and', 17474), (\"'s\", 9784), ('that', 8931)]\n",
      "end building vocab ...\n",
      "['<pad>', '<eos>', 'the', '<unk>', 'N', 'of', 'to', 'a', 'in', 'and']\n",
      "binarizing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42068/42068 [00:00<00:00, 57498.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binarizing data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3370/3370 [00:00<00:00, 57505.54it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocab(data_dir)\n",
    "trainset = LMDataset(data_dir, vocab, batch_size, 'train')\n",
    "validset = LMDataset(data_dir, vocab, batch_size, 'valid')\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_length)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size=batch_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddomgs, embedding_dim, p_drop=0.):\n",
    "        super(WordEmbedding, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddomgs, embedding_dim)\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.emb(input)\n",
    "        output = self.dropout(output)\n",
    "        return output\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers, p_drop):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.n_classes = vocab_size\n",
    "        self.d_emb = embedding_dim\n",
    "\n",
    "\n",
    "        self.word_embedding = WordEmbedding(self.n_classes, self.d_emb, p_drop=p_drop)\n",
    "        self.layers = nn.GRU(self.d_emb, hidden_dim, n_layers, dropout=p_drop, batch_first=True)\n",
    "        self.proj_layer = nn.Linear(hidden_dim, self.n_classes)\n",
    "        \n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        # self.layer2 = nn.GRU(hidden_dim, self.n_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        :param input: (bsz, seq_len)\n",
    "        :return output: (bsz, seq_len, n_voc)\n",
    "        \"\"\"\n",
    "        emb = self.word_embedding(input)\n",
    "        output, h = self.layers(emb)\n",
    "        output = self.drop(output)\n",
    "        output = self.proj_layer(output)\n",
    "        return output\n",
    "\n",
    "model = RNNLM(vocab_size=vocab.size, embedding_dim=d_emb, hidden_dim=d_hid, n_layers=n_layers, p_drop=p_drop)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = lr, # config.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # config.adam_epsilon  - default is 1e-8.\n",
    "                  )\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/10] epochs training...\n",
      "n_iter:200 loss: 7.081 ppl: 1189.649\n",
      "n_iter:400 loss: 6.560 ppl: 706.575\n",
      "n_iter:600 loss: 6.409 ppl: 607.129\n",
      "n_iter:800 loss: 6.216 ppl: 500.853\n",
      "n_iter:1000 loss: 6.143 ppl: 465.581\n",
      "n_iter:1200 loss: 6.067 ppl: 431.181\n",
      "n_iter:1400 loss: 5.968 ppl: 390.705\n",
      "n_iter:1600 loss: 5.955 ppl: 385.618\n",
      "n_iter:1800 loss: 5.838 ppl: 343.188\n",
      "n_iter:2000 loss: 5.823 ppl: 338.148\n",
      "n_iter:2200 loss: 5.733 ppl: 308.925\n",
      "n_iter:2400 loss: 5.805 ppl: 332.081\n",
      "n_iter:2600 loss: 5.768 ppl: 319.810\n",
      "n_iter:2800 loss: 5.761 ppl: 317.586\n",
      "n_iter:3000 loss: 5.857 ppl: 349.741\n",
      "n_iter:3200 loss: 5.553 ppl: 257.906\n",
      "n_iter:3400 loss: 5.725 ppl: 306.311\n",
      "n_iter:3600 loss: 5.756 ppl: 316.233\n",
      "n_iter:3800 loss: 5.702 ppl: 299.333\n",
      "n_iter:4000 loss: 5.480 ppl: 239.879\n",
      "n_iter:4200 loss: 5.693 ppl: 296.845\n",
      "n_iter:4400 loss: 5.545 ppl: 255.882\n",
      "n_iter:4600 loss: 5.409 ppl: 223.473\n",
      "n_iter:4800 loss: 5.399 ppl: 221.194\n",
      "n_iter:5000 loss: 5.530 ppl: 252.200\n",
      "n_iter:5200 loss: 5.516 ppl: 248.585\n",
      "n_iter:5400 loss: 5.578 ppl: 264.519\n",
      "n_iter:5600 loss: 5.554 ppl: 258.200\n",
      "n_iter:5800 loss: 5.650 ppl: 284.346\n",
      "n_iter:6000 loss: 5.438 ppl: 229.882\n",
      "n_iter:6200 loss: 5.594 ppl: 268.928\n",
      "n_iter:6400 loss: 5.585 ppl: 266.273\n",
      "n_iter:6600 loss: 5.512 ppl: 247.681\n",
      "n_iter:6800 loss: 5.352 ppl: 210.991\n",
      "n_iter:7000 loss: 5.487 ppl: 241.542\n",
      "n_iter:7200 loss: 5.492 ppl: 242.713\n",
      "### find best mode ### 215.04429822767386\n",
      "validation vloss: 5.371 vppl: 215.044, best ppl: 215.044\n",
      "[1/10] epochs training...\n",
      "n_iter:7400 loss: 5.529 ppl: 251.985\n",
      "n_iter:7600 loss: 5.399 ppl: 221.235\n",
      "n_iter:7800 loss: 5.520 ppl: 249.591\n",
      "n_iter:8000 loss: 5.407 ppl: 222.984\n",
      "n_iter:8200 loss: 5.386 ppl: 218.308\n",
      "n_iter:8400 loss: 5.399 ppl: 221.259\n",
      "n_iter:8600 loss: 5.209 ppl: 182.936\n",
      "n_iter:8800 loss: 5.309 ppl: 202.136\n",
      "n_iter:9000 loss: 5.283 ppl: 197.034\n",
      "n_iter:9200 loss: 5.254 ppl: 191.393\n",
      "n_iter:9400 loss: 5.213 ppl: 183.652\n",
      "n_iter:9600 loss: 5.251 ppl: 190.718\n",
      "n_iter:9800 loss: 5.255 ppl: 191.478\n",
      "n_iter:10000 loss: 5.276 ppl: 195.640\n",
      "n_iter:10200 loss: 5.346 ppl: 209.745\n",
      "n_iter:10400 loss: 5.176 ppl: 176.947\n",
      "n_iter:10600 loss: 5.219 ppl: 184.766\n",
      "n_iter:10800 loss: 5.347 ppl: 210.053\n",
      "n_iter:11000 loss: 5.388 ppl: 218.686\n",
      "n_iter:11200 loss: 5.139 ppl: 170.623\n",
      "n_iter:11400 loss: 5.182 ppl: 178.100\n",
      "n_iter:11600 loss: 5.247 ppl: 190.045\n",
      "n_iter:11800 loss: 5.036 ppl: 153.835\n",
      "n_iter:12000 loss: 5.045 ppl: 155.308\n",
      "n_iter:12200 loss: 5.167 ppl: 175.386\n",
      "n_iter:12400 loss: 5.198 ppl: 180.947\n",
      "n_iter:12600 loss: 5.185 ppl: 178.631\n",
      "n_iter:12800 loss: 5.316 ppl: 203.585\n",
      "n_iter:13000 loss: 5.254 ppl: 191.323\n",
      "n_iter:13200 loss: 5.180 ppl: 177.757\n",
      "n_iter:13400 loss: 5.218 ppl: 184.652\n",
      "n_iter:13600 loss: 5.326 ppl: 205.595\n",
      "n_iter:13800 loss: 5.218 ppl: 184.637\n",
      "n_iter:14000 loss: 5.088 ppl: 162.069\n",
      "n_iter:14200 loss: 5.256 ppl: 191.748\n",
      "n_iter:14400 loss: 5.107 ppl: 165.167\n",
      "### find best mode ### 188.07535530231033\n",
      "validation vloss: 5.237 vppl: 188.075, best ppl: 188.075\n",
      "[2/10] epochs training...\n",
      "n_iter:14600 loss: 5.363 ppl: 213.316\n",
      "n_iter:14800 loss: 5.141 ppl: 170.945\n",
      "n_iter:15000 loss: 5.298 ppl: 199.844\n",
      "n_iter:15200 loss: 5.271 ppl: 194.635\n",
      "n_iter:15400 loss: 5.177 ppl: 177.137\n",
      "n_iter:15600 loss: 5.158 ppl: 173.762\n",
      "n_iter:15800 loss: 5.009 ppl: 149.824\n",
      "n_iter:16000 loss: 5.158 ppl: 173.774\n",
      "n_iter:16200 loss: 5.066 ppl: 158.474\n",
      "n_iter:16400 loss: 5.069 ppl: 159.035\n",
      "n_iter:16600 loss: 5.046 ppl: 155.403\n",
      "n_iter:16800 loss: 4.965 ppl: 143.277\n",
      "n_iter:17000 loss: 5.020 ppl: 151.354\n",
      "n_iter:17200 loss: 5.079 ppl: 160.652\n",
      "n_iter:17400 loss: 5.142 ppl: 171.049\n",
      "n_iter:17600 loss: 5.178 ppl: 177.291\n",
      "n_iter:17800 loss: 4.882 ppl: 131.956\n",
      "n_iter:18000 loss: 5.144 ppl: 171.409\n",
      "n_iter:18200 loss: 5.212 ppl: 183.377\n",
      "n_iter:18400 loss: 5.037 ppl: 153.981\n",
      "n_iter:18600 loss: 4.889 ppl: 132.825\n",
      "n_iter:18800 loss: 5.142 ppl: 171.099\n",
      "n_iter:19000 loss: 4.973 ppl: 144.479\n",
      "n_iter:19200 loss: 4.805 ppl: 122.171\n",
      "n_iter:19400 loss: 4.903 ppl: 134.722\n",
      "n_iter:19600 loss: 4.995 ppl: 147.657\n",
      "n_iter:19800 loss: 5.050 ppl: 156.036\n",
      "n_iter:20000 loss: 5.168 ppl: 175.555\n",
      "n_iter:20200 loss: 5.007 ppl: 149.480\n",
      "n_iter:20400 loss: 5.101 ppl: 164.246\n",
      "n_iter:20600 loss: 5.021 ppl: 151.637\n",
      "n_iter:20800 loss: 5.192 ppl: 179.875\n",
      "n_iter:21000 loss: 5.100 ppl: 164.072\n",
      "n_iter:21200 loss: 4.962 ppl: 142.947\n",
      "n_iter:21400 loss: 5.020 ppl: 151.363\n",
      "n_iter:21600 loss: 4.949 ppl: 140.991\n",
      "### find best mode ### 176.9629562000106\n",
      "validation vloss: 5.176 vppl: 176.963, best ppl: 176.963\n",
      "[3/10] epochs training...\n",
      "n_iter:21800 loss: 5.176 ppl: 176.981\n",
      "n_iter:22000 loss: 5.101 ppl: 164.149\n",
      "n_iter:22200 loss: 5.108 ppl: 165.408\n",
      "n_iter:22400 loss: 5.159 ppl: 173.970\n",
      "n_iter:22600 loss: 5.069 ppl: 159.014\n",
      "n_iter:22800 loss: 4.996 ppl: 147.784\n",
      "n_iter:23000 loss: 5.025 ppl: 152.219\n",
      "n_iter:23200 loss: 4.901 ppl: 134.395\n",
      "n_iter:23400 loss: 4.949 ppl: 141.019\n",
      "n_iter:23600 loss: 4.936 ppl: 139.165\n",
      "n_iter:23800 loss: 4.911 ppl: 135.730\n",
      "n_iter:24000 loss: 4.848 ppl: 127.514\n",
      "n_iter:24200 loss: 4.908 ppl: 135.416\n",
      "n_iter:24400 loss: 4.941 ppl: 139.950\n",
      "n_iter:24600 loss: 4.959 ppl: 142.408\n",
      "n_iter:24800 loss: 5.095 ppl: 163.251\n",
      "n_iter:25000 loss: 4.730 ppl: 113.298\n",
      "n_iter:25200 loss: 5.010 ppl: 149.838\n",
      "n_iter:25400 loss: 5.050 ppl: 155.978\n",
      "n_iter:25600 loss: 5.000 ppl: 148.401\n",
      "n_iter:25800 loss: 4.734 ppl: 113.731\n",
      "n_iter:26000 loss: 5.041 ppl: 154.620\n",
      "n_iter:26200 loss: 4.882 ppl: 131.896\n",
      "n_iter:26400 loss: 4.717 ppl: 111.854\n",
      "n_iter:26600 loss: 4.768 ppl: 117.655\n",
      "n_iter:26800 loss: 4.935 ppl: 139.143\n",
      "n_iter:27000 loss: 4.888 ppl: 132.684\n",
      "n_iter:27200 loss: 4.950 ppl: 141.105\n",
      "n_iter:27400 loss: 4.935 ppl: 139.141\n",
      "n_iter:27600 loss: 5.071 ppl: 159.296\n",
      "n_iter:27800 loss: 4.828 ppl: 124.948\n",
      "n_iter:28000 loss: 5.047 ppl: 155.580\n",
      "n_iter:28200 loss: 5.015 ppl: 150.641\n",
      "n_iter:28400 loss: 4.957 ppl: 142.118\n",
      "n_iter:28600 loss: 4.774 ppl: 118.381\n",
      "n_iter:28800 loss: 4.919 ppl: 136.894\n",
      "n_iter:29000 loss: 4.955 ppl: 141.814\n",
      "### find best mode ### 170.76569681560468\n",
      "validation vloss: 5.140 vppl: 170.766, best ppl: 170.766\n",
      "[4/10] epochs training...\n",
      "n_iter:29200 loss: 5.041 ppl: 154.623\n",
      "n_iter:29400 loss: 4.965 ppl: 143.285\n",
      "n_iter:29600 loss: 5.082 ppl: 161.062\n",
      "n_iter:29800 loss: 4.991 ppl: 147.104\n",
      "n_iter:30000 loss: 4.939 ppl: 139.655\n",
      "n_iter:30200 loss: 4.992 ppl: 147.204\n",
      "n_iter:30400 loss: 4.746 ppl: 115.106\n",
      "n_iter:30600 loss: 4.874 ppl: 130.908\n",
      "n_iter:30800 loss: 4.866 ppl: 129.827\n",
      "n_iter:31000 loss: 4.846 ppl: 127.184\n",
      "n_iter:31200 loss: 4.757 ppl: 116.429\n",
      "n_iter:31400 loss: 4.801 ppl: 121.622\n",
      "n_iter:31600 loss: 4.821 ppl: 124.076\n",
      "n_iter:31800 loss: 4.884 ppl: 132.105\n",
      "n_iter:32000 loss: 4.909 ppl: 135.482\n",
      "n_iter:32200 loss: 4.750 ppl: 115.545\n",
      "n_iter:32400 loss: 4.824 ppl: 124.499\n",
      "n_iter:32600 loss: 4.972 ppl: 144.274\n",
      "n_iter:32800 loss: 5.006 ppl: 149.239\n",
      "n_iter:33000 loss: 4.709 ppl: 110.928\n",
      "n_iter:33200 loss: 4.802 ppl: 121.757\n",
      "n_iter:33400 loss: 4.860 ppl: 129.082\n",
      "n_iter:33600 loss: 4.640 ppl: 103.510\n",
      "n_iter:33800 loss: 4.678 ppl: 107.583\n",
      "n_iter:34000 loss: 4.811 ppl: 122.867\n",
      "n_iter:34200 loss: 4.802 ppl: 121.746\n",
      "n_iter:34400 loss: 4.804 ppl: 122.054\n",
      "n_iter:34600 loss: 4.929 ppl: 138.188\n",
      "n_iter:34800 loss: 4.909 ppl: 135.479\n",
      "n_iter:35000 loss: 4.787 ppl: 119.932\n",
      "n_iter:35200 loss: 4.858 ppl: 128.732\n",
      "n_iter:35400 loss: 4.994 ppl: 147.557\n",
      "n_iter:35600 loss: 4.886 ppl: 132.456\n",
      "n_iter:35800 loss: 4.686 ppl: 108.370\n",
      "n_iter:36000 loss: 4.888 ppl: 132.712\n",
      "n_iter:36200 loss: 4.788 ppl: 120.029\n",
      "### find best mode ### 167.98174242815782\n",
      "validation vloss: 5.124 vppl: 167.982, best ppl: 167.982\n",
      "[5/10] epochs training...\n",
      "n_iter:36400 loss: 5.012 ppl: 150.181\n",
      "n_iter:36600 loss: 4.842 ppl: 126.769\n",
      "n_iter:36800 loss: 5.007 ppl: 149.451\n",
      "n_iter:37000 loss: 4.971 ppl: 144.136\n",
      "n_iter:37200 loss: 4.884 ppl: 132.136\n",
      "n_iter:37400 loss: 4.859 ppl: 128.889\n",
      "n_iter:37600 loss: 4.681 ppl: 107.856\n",
      "n_iter:37800 loss: 4.871 ppl: 130.477\n",
      "n_iter:38000 loss: 4.739 ppl: 114.358\n",
      "n_iter:38200 loss: 4.784 ppl: 119.631\n",
      "n_iter:38400 loss: 4.788 ppl: 120.106\n",
      "n_iter:38600 loss: 4.632 ppl: 102.761\n",
      "n_iter:38800 loss: 4.728 ppl: 113.046\n",
      "n_iter:39000 loss: 4.797 ppl: 121.168\n",
      "n_iter:39200 loss: 4.843 ppl: 126.895\n",
      "n_iter:39400 loss: 4.828 ppl: 124.899\n",
      "n_iter:39600 loss: 4.595 ppl: 98.994\n",
      "n_iter:39800 loss: 4.866 ppl: 129.820\n",
      "n_iter:40000 loss: 4.936 ppl: 139.175\n",
      "n_iter:40200 loss: 4.734 ppl: 113.751\n",
      "n_iter:40400 loss: 4.622 ppl: 101.703\n",
      "n_iter:40600 loss: 4.863 ppl: 129.349\n",
      "n_iter:40800 loss: 4.641 ppl: 103.618\n",
      "n_iter:41000 loss: 4.553 ppl: 94.877\n",
      "n_iter:41200 loss: 4.627 ppl: 102.236\n",
      "n_iter:41400 loss: 4.699 ppl: 109.801\n",
      "n_iter:41600 loss: 4.771 ppl: 118.061\n",
      "n_iter:41800 loss: 4.891 ppl: 133.147\n",
      "n_iter:42000 loss: 4.740 ppl: 114.401\n",
      "n_iter:42200 loss: 4.809 ppl: 122.586\n",
      "n_iter:42400 loss: 4.760 ppl: 116.758\n",
      "n_iter:42600 loss: 4.935 ppl: 139.064\n",
      "n_iter:42800 loss: 4.824 ppl: 124.493\n",
      "n_iter:43000 loss: 4.675 ppl: 107.180\n",
      "n_iter:43200 loss: 4.753 ppl: 115.882\n",
      "n_iter:43400 loss: 4.669 ppl: 106.565\n",
      "### find best mode ### 164.72863758448744\n",
      "validation vloss: 5.104 vppl: 164.729, best ppl: 164.729\n",
      "[6/10] epochs training...\n",
      "n_iter:43600 loss: 4.940 ppl: 139.782\n",
      "n_iter:43800 loss: 4.811 ppl: 122.844\n",
      "n_iter:44000 loss: 4.912 ppl: 135.969\n",
      "n_iter:44200 loss: 4.928 ppl: 138.103\n",
      "n_iter:44400 loss: 4.827 ppl: 124.886\n",
      "n_iter:44600 loss: 4.762 ppl: 116.977\n",
      "n_iter:44800 loss: 4.774 ppl: 118.418\n",
      "n_iter:45000 loss: 4.705 ppl: 110.445\n",
      "n_iter:45200 loss: 4.700 ppl: 109.951\n",
      "n_iter:45400 loss: 4.722 ppl: 112.368\n",
      "n_iter:45600 loss: 4.688 ppl: 108.596\n",
      "n_iter:45800 loss: 4.615 ppl: 101.016\n",
      "n_iter:46000 loss: 4.678 ppl: 107.593\n",
      "n_iter:46200 loss: 4.702 ppl: 110.137\n",
      "n_iter:46400 loss: 4.735 ppl: 113.827\n",
      "n_iter:46600 loss: 4.864 ppl: 129.555\n",
      "n_iter:46800 loss: 4.481 ppl: 88.292\n",
      "n_iter:47000 loss: 4.803 ppl: 121.866\n",
      "n_iter:47200 loss: 4.825 ppl: 124.592\n",
      "n_iter:47400 loss: 4.749 ppl: 115.490\n",
      "n_iter:47600 loss: 4.507 ppl: 90.622\n",
      "n_iter:47800 loss: 4.823 ppl: 124.320\n",
      "n_iter:48000 loss: 4.661 ppl: 105.695\n",
      "n_iter:48200 loss: 4.460 ppl: 86.478\n",
      "n_iter:48400 loss: 4.547 ppl: 94.355\n",
      "n_iter:48600 loss: 4.714 ppl: 111.537\n",
      "n_iter:48800 loss: 4.662 ppl: 105.863\n",
      "n_iter:49000 loss: 4.714 ppl: 111.552\n",
      "n_iter:49200 loss: 4.724 ppl: 112.660\n",
      "n_iter:49400 loss: 4.834 ppl: 125.660\n",
      "n_iter:49600 loss: 4.610 ppl: 100.475\n",
      "n_iter:49800 loss: 4.875 ppl: 130.989\n",
      "n_iter:50000 loss: 4.765 ppl: 117.323\n",
      "n_iter:50200 loss: 4.740 ppl: 114.380\n",
      "n_iter:50400 loss: 4.576 ppl: 97.096\n",
      "n_iter:50600 loss: 4.660 ppl: 105.601\n",
      "n_iter:50800 loss: 4.775 ppl: 118.472\n",
      "### find best mode ### 163.14540671171594\n",
      "validation vloss: 5.095 vppl: 163.145, best ppl: 163.145\n",
      "[7/10] epochs training...\n",
      "n_iter:51000 loss: 4.847 ppl: 127.397\n",
      "n_iter:51200 loss: 4.777 ppl: 118.795\n",
      "n_iter:51400 loss: 4.881 ppl: 131.772\n",
      "n_iter:51600 loss: 4.781 ppl: 119.169\n",
      "n_iter:51800 loss: 4.754 ppl: 116.085\n",
      "n_iter:52000 loss: 4.792 ppl: 120.518\n",
      "n_iter:52200 loss: 4.556 ppl: 95.205\n",
      "n_iter:52400 loss: 4.691 ppl: 109.010\n",
      "n_iter:52600 loss: 4.685 ppl: 108.314\n",
      "n_iter:52800 loss: 4.652 ppl: 104.831\n",
      "n_iter:53000 loss: 4.551 ppl: 94.735\n",
      "n_iter:53200 loss: 4.605 ppl: 99.967\n",
      "n_iter:53400 loss: 4.641 ppl: 103.694\n",
      "n_iter:53600 loss: 4.704 ppl: 110.438\n",
      "n_iter:53800 loss: 4.703 ppl: 110.265\n",
      "n_iter:54000 loss: 4.538 ppl: 93.516\n",
      "n_iter:54200 loss: 4.653 ppl: 104.890\n",
      "n_iter:54400 loss: 4.793 ppl: 120.646\n",
      "n_iter:54600 loss: 4.806 ppl: 122.284\n",
      "n_iter:54800 loss: 4.514 ppl: 91.296\n",
      "n_iter:55000 loss: 4.642 ppl: 103.711\n",
      "n_iter:55200 loss: 4.648 ppl: 104.425\n",
      "n_iter:55400 loss: 4.463 ppl: 86.740\n",
      "n_iter:55600 loss: 4.489 ppl: 89.075\n",
      "n_iter:55800 loss: 4.613 ppl: 100.740\n",
      "n_iter:56000 loss: 4.606 ppl: 100.111\n",
      "n_iter:56200 loss: 4.642 ppl: 103.711\n",
      "n_iter:56400 loss: 4.705 ppl: 110.523\n",
      "n_iter:56600 loss: 4.763 ppl: 117.098\n",
      "n_iter:56800 loss: 4.589 ppl: 98.418\n",
      "n_iter:57000 loss: 4.697 ppl: 109.669\n",
      "n_iter:57200 loss: 4.791 ppl: 120.418\n",
      "n_iter:57400 loss: 4.708 ppl: 110.862\n",
      "n_iter:57600 loss: 4.467 ppl: 87.126\n",
      "n_iter:57800 loss: 4.720 ppl: 112.183\n",
      "n_iter:58000 loss: 4.607 ppl: 100.157\n",
      "### find best mode ### 161.79597586808222\n",
      "validation vloss: 5.086 vppl: 161.796, best ppl: 161.796\n",
      "[8/10] epochs training...\n",
      "n_iter:58200 loss: 4.830 ppl: 125.229\n",
      "n_iter:58400 loss: 4.678 ppl: 107.556\n",
      "n_iter:58600 loss: 4.851 ppl: 127.842\n",
      "n_iter:58800 loss: 4.806 ppl: 122.275\n",
      "n_iter:59000 loss: 4.721 ppl: 112.328\n",
      "n_iter:59200 loss: 4.717 ppl: 111.853\n",
      "n_iter:59400 loss: 4.508 ppl: 90.751\n",
      "n_iter:59600 loss: 4.712 ppl: 111.330\n",
      "n_iter:59800 loss: 4.583 ppl: 97.761\n",
      "n_iter:60000 loss: 4.642 ppl: 103.782\n",
      "n_iter:60200 loss: 4.623 ppl: 101.774\n",
      "n_iter:60400 loss: 4.454 ppl: 85.943\n",
      "n_iter:60600 loss: 4.589 ppl: 98.349\n",
      "n_iter:60800 loss: 4.630 ppl: 102.562\n",
      "n_iter:61000 loss: 4.682 ppl: 107.973\n",
      "n_iter:61200 loss: 4.624 ppl: 101.908\n",
      "n_iter:61400 loss: 4.473 ppl: 87.616\n",
      "n_iter:61600 loss: 4.708 ppl: 110.813\n",
      "n_iter:61800 loss: 4.783 ppl: 119.428\n",
      "n_iter:62000 loss: 4.559 ppl: 95.445\n",
      "n_iter:62200 loss: 4.494 ppl: 89.510\n",
      "n_iter:62400 loss: 4.691 ppl: 108.927\n",
      "n_iter:62600 loss: 4.462 ppl: 86.640\n",
      "n_iter:62800 loss: 4.388 ppl: 80.469\n",
      "n_iter:63000 loss: 4.494 ppl: 89.441\n",
      "n_iter:63200 loss: 4.584 ppl: 97.876\n",
      "n_iter:63400 loss: 4.581 ppl: 97.622\n",
      "n_iter:63600 loss: 4.730 ppl: 113.269\n",
      "n_iter:63800 loss: 4.593 ppl: 98.760\n",
      "n_iter:64000 loss: 4.647 ppl: 104.312\n",
      "n_iter:64200 loss: 4.605 ppl: 99.987\n",
      "n_iter:64400 loss: 4.772 ppl: 118.121\n",
      "n_iter:64600 loss: 4.656 ppl: 105.245\n",
      "n_iter:64800 loss: 4.504 ppl: 90.336\n",
      "n_iter:65000 loss: 4.610 ppl: 100.533\n",
      "n_iter:65200 loss: 4.521 ppl: 91.913\n",
      "### find best mode ### 161.1774720335262\n",
      "validation vloss: 5.083 vppl: 161.177, best ppl: 161.177\n",
      "[9/10] epochs training...\n",
      "n_iter:65400 loss: 4.773 ppl: 118.231\n",
      "n_iter:65600 loss: 4.653 ppl: 104.867\n",
      "n_iter:65800 loss: 4.785 ppl: 119.737\n",
      "n_iter:66000 loss: 4.785 ppl: 119.643\n",
      "n_iter:66200 loss: 4.699 ppl: 109.885\n",
      "n_iter:66400 loss: 4.628 ppl: 102.344\n",
      "n_iter:66600 loss: 4.600 ppl: 99.518\n",
      "n_iter:66800 loss: 4.602 ppl: 99.664\n",
      "n_iter:67000 loss: 4.564 ppl: 95.964\n",
      "n_iter:67200 loss: 4.580 ppl: 97.495\n",
      "n_iter:67400 loss: 4.545 ppl: 94.181\n",
      "n_iter:67600 loss: 4.459 ppl: 86.417\n",
      "n_iter:67800 loss: 4.529 ppl: 92.685\n",
      "n_iter:68000 loss: 4.551 ppl: 94.719\n",
      "n_iter:68200 loss: 4.593 ppl: 98.782\n",
      "n_iter:68400 loss: 4.722 ppl: 112.394\n",
      "n_iter:68600 loss: 4.340 ppl: 76.743\n",
      "n_iter:68800 loss: 4.656 ppl: 105.234\n",
      "n_iter:69000 loss: 4.697 ppl: 109.634\n",
      "n_iter:69200 loss: 4.604 ppl: 99.922\n",
      "n_iter:69400 loss: 4.366 ppl: 78.718\n",
      "n_iter:69600 loss: 4.673 ppl: 106.971\n",
      "n_iter:69800 loss: 4.506 ppl: 90.548\n",
      "n_iter:70000 loss: 4.319 ppl: 75.131\n",
      "n_iter:70200 loss: 4.405 ppl: 81.885\n",
      "n_iter:70400 loss: 4.603 ppl: 99.827\n",
      "n_iter:70600 loss: 4.516 ppl: 91.434\n",
      "n_iter:70800 loss: 4.597 ppl: 99.216\n",
      "n_iter:71000 loss: 4.579 ppl: 97.438\n",
      "n_iter:71200 loss: 4.688 ppl: 108.680\n",
      "n_iter:71400 loss: 4.494 ppl: 89.476\n",
      "n_iter:71600 loss: 4.745 ppl: 115.018\n",
      "n_iter:71800 loss: 4.617 ppl: 101.214\n",
      "n_iter:72000 loss: 4.565 ppl: 96.020\n",
      "n_iter:72200 loss: 4.461 ppl: 86.584\n",
      "n_iter:72400 loss: 4.497 ppl: 89.719\n",
      "n_iter:72600 loss: 4.644 ppl: 103.943\n",
      "### find best mode ### 159.7865747183466\n",
      "validation vloss: 5.074 vppl: 159.787, best ppl: 159.787\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_iter, train_loss, valid_loss, best_ppl = 0, 0., 0., float('inf')\n",
    "for ep in range(epochs):\n",
    "    print(f\"[{ep}/{epochs}] epochs training...\")\n",
    "    \n",
    "    # train\n",
    "    model.train()\n",
    "    for batch in trainloader:\n",
    "        n_iter += 1\n",
    "        batch = batch.transpose(1, 0).contiguous().to(device)\n",
    "        \n",
    "        target = batch[:, 1:].clone()\n",
    "        logits = model(batch[:, :-1])\n",
    "        loss = F.cross_entropy(logits.reshape(-1, vocab.size), target.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if n_iter % interval_print == 0:\n",
    "            train_loss /= interval_print\n",
    "            train_ppl = math.exp(train_loss)\n",
    "            print(f\"n_iter:{n_iter} loss: {train_loss:0.3f} ppl: {train_ppl:0.3f}\")\n",
    "            train_loss = 0\n",
    "            \n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    for step, batch in enumerate(validloader, 1):\n",
    "        batch = batch.transpose(1, 0).to(device)\n",
    "        with torch.no_grad():\n",
    "            target = batch[:, 1:].clone()\n",
    "            logits = model(batch[:, :-1])\n",
    "            loss = F.cross_entropy(logits.reshape(-1, vocab.size), target.reshape(-1))\n",
    "            valid_loss += loss.item()\n",
    "    valid_loss = valid_loss/step\n",
    "    valid_ppl = math.exp(valid_loss)\n",
    "\n",
    "    if valid_ppl < best_ppl:\n",
    "        best_ppl = valid_ppl\n",
    "        torch.save(model, \"rnnlm-best.pth\")\n",
    "        print(\"### find best mode ###\", best_ppl)\n",
    "\n",
    "    print(f\"validation vloss: {valid_loss:0.3f} vppl: {valid_ppl:0.3f}, best ppl: {best_ppl:0.3f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warmup: i just think\n",
      "generated: i just think i 'm not going to see the <unk> of the <unk> <unk> <eos> i 'm not going to see the\n"
     ]
    }
   ],
   "source": [
    "max_len = 20\n",
    "generated = warmup = \"i just think\".lower()\n",
    "best_model = torch.load('./rnnlm-best.pth')\n",
    "best_model.eval()\n",
    "\n",
    "for _ in range(max_len):\n",
    "    inputs = torch.tensor([vocab.encode_line(generated, add_eos=False)]).to(device)\n",
    "    logits = F.softmax(best_model(inputs), dim=-1)[:, -1, :]\n",
    "    predicted_tok = vocab.id2tok[logits.argmax(-1).item()]\n",
    "    generated = \" \".join([generated, predicted_tok])\n",
    "\n",
    "print(f\"warmup: {warmup}\")\n",
    "print(f\"generated: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e235b1f40ab561dc697a518ea04835f082f50dd8c1dee948e78ceb42cbdb5e37"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
